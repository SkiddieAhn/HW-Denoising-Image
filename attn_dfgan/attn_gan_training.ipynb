{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model.generator_cbam import AttnUNet\n",
    "from model.discriminator import PixelDiscriminator\n",
    "from pytorch_model_summary import summary\n",
    "from dataset import *\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0480) tensor(3.7225)\n",
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_path = 'data/trainset.npy'\n",
    "valid_path = 'data/validset.npy'\n",
    "\n",
    "train_set = npyDataset(npy_file=train_path, resize_h=256, resize_w=256)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_set = npyDataset(npy_file=valid_path, resize_h=256, resize_w=256)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataiter = iter(train_set)\n",
    "targets, inputs  = next(dataiter)\n",
    "print(torch.min(targets), torch.max(targets))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 256\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "\n",
    "def initialize_weights(m):\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "      if m.bias is not None:\n",
    "          nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.BatchNorm2d):\n",
    "      nn.init.constant_(m.weight.data, 1)\n",
    "      nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.Linear):\n",
    "      nn.init.kaiming_uniform_(m.weight.data)\n",
    "      nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          inconv-1     [16, 32, 256, 256]           9,632           9,632\n",
      "            down-2     [16, 64, 128, 128]          56,232          56,232\n",
      "            down-3      [16, 128, 64, 64]         223,980         223,980\n",
      "            down-4      [16, 256, 32, 32]         894,324         894,324\n",
      "            down-5      [16, 512, 16, 16]       3,574,404       3,574,404\n",
      "              up-6      [16, 256, 32, 32]       2,303,604       2,303,604\n",
      "          Conv2d-7        [16, 1, 32, 32]              65              65\n",
      "              up-8      [16, 128, 64, 64]         576,364         576,364\n",
      "          Conv2d-9        [16, 1, 64, 64]              17              17\n",
      "             up-10     [16, 64, 128, 128]         144,360         144,360\n",
      "         Conv2d-11      [16, 1, 128, 128]               5               5\n",
      "             up-12     [16, 32, 256, 256]          36,262          36,262\n",
      "         Conv2d-13      [16, 1, 256, 256]             289             289\n",
      "==========================================================================\n",
      "Total params: 7,819,538\n",
      "Trainable params: 7,819,538\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([16, 1, 256, 256])\n",
      "output: torch.Size([16, 1, 256, 256])\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sha/anaconda3/envs/dvaa/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttnUNet(\n",
       "  (inc): inconv(\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down1): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=512, out_features=32, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=32, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): up(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): up(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): up(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): up(\n",
       "    (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attn): CBAM(\n",
       "      (ChannelGate): ChannelGate(\n",
       "        (mlp): Sequential(\n",
       "          (0): Flatten()\n",
       "          (1): Linear(in_features=32, out_features=2, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=2, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (SpatialGate): SpatialGate(\n",
       "        (compress): ChannelPool()\n",
       "        (spatial): BasicConv(\n",
       "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (size_down8): Conv2d(1, 1, kernel_size=(8, 8), stride=(8, 8))\n",
       "  (size_down4): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (size_down2): Conv2d(1, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = AttnUNet(in_channels, out_channels).to(device)\n",
    "\n",
    "x = torch.ones([batch_size, in_channels, resolution, resolution]).cuda()\n",
    "\n",
    "print(summary(generator,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',generator(x).shape)\n",
    "print('===================================')\n",
    "\n",
    "generator.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Conv2d-1     [16, 128, 128, 128]           4,224           4,224\n",
      "       LeakyReLU-2     [16, 128, 128, 128]               0               0\n",
      "          Conv2d-3       [16, 256, 64, 64]         524,544         524,544\n",
      "       LeakyReLU-4       [16, 256, 64, 64]               0               0\n",
      "          Conv2d-5       [16, 512, 32, 32]       2,097,664       2,097,664\n",
      "       LeakyReLU-6       [16, 512, 32, 32]               0               0\n",
      "          Conv2d-7       [16, 512, 31, 31]       4,194,816       4,194,816\n",
      "       LeakyReLU-8       [16, 512, 31, 31]               0               0\n",
      "          Conv2d-9         [16, 1, 30, 30]           8,193           8,193\n",
      "===========================================================================\n",
      "Total params: 6,829,441\n",
      "Trainable params: 6,829,441\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "input: torch.Size([16, 2, 256, 256])\n",
      "output: torch.Size([16, 1, 30, 30])\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelDiscriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (out_conv): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = PixelDiscriminator(2).to(device)\n",
    "\n",
    "x = torch.ones([batch_size, 2, resolution, resolution]).cuda()\n",
    "\n",
    "print(summary(discriminator,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',discriminator(x).shape)\n",
    "print('===================================')\n",
    "\n",
    "discriminator.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = 0.0002\n",
    "d_lr = 0.00002\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "# G loss\n",
    "coefs = [1, 1, 1, 1, 0.05]\n",
    "mse_loss_1 = Intensity_Loss().to(device)\n",
    "mse_loss_2 = Intensity_Loss().to(device)\n",
    "gd_loss_1 = Gradient_Loss(1).to(device)\n",
    "gd_loss_2 = Gradient_Loss(1).to(device)\n",
    "adv_loss = Adversarial_Loss().to(device)\n",
    "\n",
    "# D loss\n",
    "disc_loss = Discriminate_Loss().to(device)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=d_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './weight/attn_gan_generator'\n",
    "BEST_SAVE_PATH = './weight/attn_gan_generator_best.pth'\n",
    "LAST_SAVE_PATH = './weight/attn_gan_generator_last.pth'\n",
    "train_g_losses = []\n",
    "valid_g_losses = []\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getG_loss(out, sub, out_target, sub_target, fake, coefs): \n",
    "    '''\n",
    "    coefs = [1, 1, 1, 1, 0.05]\n",
    "    '''\n",
    "\n",
    "    mse_out = mse_loss_1(out, out_target)\n",
    "    mse_sub = mse_loss_2(sub, sub_target)\n",
    "    gdl_out = gd_loss_1(out, out_target)\n",
    "    gdl_sub = gd_loss_2(sub, sub_target)\n",
    "    adl = adv_loss(fake)\n",
    "\n",
    "    loss_gen = coefs[0] * mse_out + \\\n",
    "               coefs[1] * mse_sub + \\\n",
    "               coefs[2] * gdl_out + \\\n",
    "               coefs[3] * gdl_sub + \\\n",
    "               coefs[4] * adl\n",
    "    \n",
    "    return loss_gen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getD_loss(real, fake):\n",
    "    return disc_loss(real, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] G Loss: 0.8386968374252319 / D Loss: 0.19258534908294678\n",
      "[100] G Loss: 0.5604763627052307 / D Loss: 0.19133451581001282\n",
      "-------------------------------------------------------------------\n",
      "[1/200] Train G Loss: 2.21312\t\n",
      "[1/200] Valiation G Loss: 0.58083\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.3609579801559448 / D Loss: 0.15433377027511597\n",
      "[100] G Loss: 0.32713523507118225 / D Loss: 0.15112875401973724\n",
      "-------------------------------------------------------------------\n",
      "[2/200] Train G Loss: 0.37026\t\n",
      "[2/200] Valiation G Loss: 0.39395\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.26477083563804626 / D Loss: 0.1990690529346466\n",
      "[100] G Loss: 0.23758062720298767 / D Loss: 0.12567958235740662\n",
      "-------------------------------------------------------------------\n",
      "[3/200] Train G Loss: 0.26146\t\n",
      "[3/200] Valiation G Loss: 0.31092\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.22393997013568878 / D Loss: 0.21705345809459686\n",
      "[100] G Loss: 0.2069537341594696 / D Loss: 0.20457535982131958\n",
      "-------------------------------------------------------------------\n",
      "[4/200] Train G Loss: 0.21137\t\n",
      "[4/200] Valiation G Loss: 0.27732\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.2036115974187851 / D Loss: 0.15615513920783997\n",
      "[100] G Loss: 0.17939114570617676 / D Loss: 0.25004053115844727\n",
      "-------------------------------------------------------------------\n",
      "[5/200] Train G Loss: 0.18887\t\n",
      "[5/200] Valiation G Loss: 0.26431\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.16795304417610168 / D Loss: 0.24746708571910858\n",
      "[100] G Loss: 0.17946529388427734 / D Loss: 0.2651674449443817\n",
      "-------------------------------------------------------------------\n",
      "[6/200] Train G Loss: 0.17575\t\n",
      "[6/200] Valiation G Loss: 0.24690\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15806269645690918 / D Loss: 0.2202928215265274\n",
      "[100] G Loss: 0.1602487564086914 / D Loss: 0.22705024480819702\n",
      "-------------------------------------------------------------------\n",
      "[7/200] Train G Loss: 0.16774\t\n",
      "[7/200] Valiation G Loss: 0.23546\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1455051600933075 / D Loss: 0.29881876707077026\n",
      "[100] G Loss: 0.15174829959869385 / D Loss: 0.2948266863822937\n",
      "-------------------------------------------------------------------\n",
      "[8/200] Train G Loss: 0.16201\t\n",
      "[8/200] Valiation G Loss: 0.23000\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.16267986595630646 / D Loss: 0.22589552402496338\n",
      "[100] G Loss: 0.14241190254688263 / D Loss: 0.2534452974796295\n",
      "-------------------------------------------------------------------\n",
      "[9/200] Train G Loss: 0.15445\t\n",
      "[9/200] Valiation G Loss: 0.22133\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14684295654296875 / D Loss: 0.2369920015335083\n",
      "[100] G Loss: 0.14147979021072388 / D Loss: 0.20265206694602966\n",
      "-------------------------------------------------------------------\n",
      "[10/200] Train G Loss: 0.15143\t\n",
      "<< Best model save at [10] epoch! >>\n",
      "<< model save at [10] epoch! >>\n",
      "[10/200] Valiation G Loss: 0.21455\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14715024828910828 / D Loss: 0.2419099509716034\n",
      "[100] G Loss: 0.1521051824092865 / D Loss: 0.23219208419322968\n",
      "-------------------------------------------------------------------\n",
      "[11/200] Train G Loss: 0.14776\t\n",
      "<< Best model save at [11] epoch! >>\n",
      "[11/200] Valiation G Loss: 0.21189\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15066039562225342 / D Loss: 0.2072772979736328\n",
      "[100] G Loss: 0.14882266521453857 / D Loss: 0.18490061163902283\n",
      "-------------------------------------------------------------------\n",
      "[12/200] Train G Loss: 0.14492\t\n",
      "<< Best model save at [12] epoch! >>\n",
      "[12/200] Valiation G Loss: 0.20270\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15063776075839996 / D Loss: 0.13615557551383972\n",
      "[100] G Loss: 0.15319529175758362 / D Loss: 0.1593339741230011\n",
      "-------------------------------------------------------------------\n",
      "[13/200] Train G Loss: 0.14336\t\n",
      "<< Best model save at [13] epoch! >>\n",
      "[13/200] Valiation G Loss: 0.20249\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14627023041248322 / D Loss: 0.08856082707643509\n",
      "[100] G Loss: 0.14184677600860596 / D Loss: 0.12231864035129547\n",
      "-------------------------------------------------------------------\n",
      "[14/200] Train G Loss: 0.14485\t\n",
      "<< Best model save at [14] epoch! >>\n",
      "[14/200] Valiation G Loss: 0.20093\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1385817974805832 / D Loss: 0.20266935229301453\n",
      "[100] G Loss: 0.15970616042613983 / D Loss: 0.042724259197711945\n",
      "-------------------------------------------------------------------\n",
      "[15/200] Train G Loss: 0.14702\t\n",
      "<< model save at [15] epoch! >>\n",
      "[15/200] Valiation G Loss: 0.20672\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13243895769119263 / D Loss: 0.18368834257125854\n",
      "[100] G Loss: 0.14000478386878967 / D Loss: 0.11220571398735046\n",
      "-------------------------------------------------------------------\n",
      "[16/200] Train G Loss: 0.14225\t\n",
      "<< Best model save at [16] epoch! >>\n",
      "[16/200] Valiation G Loss: 0.19542\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13455398380756378 / D Loss: 0.08380160480737686\n",
      "[100] G Loss: 0.13066647946834564 / D Loss: 0.23732174932956696\n",
      "-------------------------------------------------------------------\n",
      "[17/200] Train G Loss: 0.14320\t\n",
      "[17/200] Valiation G Loss: 0.19846\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14421039819717407 / D Loss: 0.11330415308475494\n",
      "[100] G Loss: 0.1337224245071411 / D Loss: 0.23025965690612793\n",
      "-------------------------------------------------------------------\n",
      "[18/200] Train G Loss: 0.14315\t\n",
      "[18/200] Valiation G Loss: 0.20029\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15928971767425537 / D Loss: 0.07280415296554565\n",
      "[100] G Loss: 0.1355292797088623 / D Loss: 0.08755207061767578\n",
      "-------------------------------------------------------------------\n",
      "[19/200] Train G Loss: 0.14333\t\n",
      "<< Best model save at [19] epoch! >>\n",
      "[19/200] Valiation G Loss: 0.18856\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13833671808242798 / D Loss: 0.03465968370437622\n",
      "[100] G Loss: 0.14143261313438416 / D Loss: 0.05864878371357918\n",
      "-------------------------------------------------------------------\n",
      "[20/200] Train G Loss: 0.14315\t\n",
      "<< Best model save at [20] epoch! >>\n",
      "<< model save at [20] epoch! >>\n",
      "[20/200] Valiation G Loss: 0.18818\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14499883353710175 / D Loss: 0.08977949619293213\n",
      "[100] G Loss: 0.14301414787769318 / D Loss: 0.04326651990413666\n",
      "-------------------------------------------------------------------\n",
      "[21/200] Train G Loss: 0.14340\t\n",
      "[21/200] Valiation G Loss: 0.19330\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14987261593341827 / D Loss: 0.047213610261678696\n",
      "[100] G Loss: 0.1422310173511505 / D Loss: 0.01818494126200676\n",
      "-------------------------------------------------------------------\n",
      "[22/200] Train G Loss: 0.14387\t\n",
      "[22/200] Valiation G Loss: 0.19498\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14418072998523712 / D Loss: 0.06593171507120132\n",
      "[100] G Loss: 0.12464941293001175 / D Loss: 0.21466143429279327\n",
      "-------------------------------------------------------------------\n",
      "[23/200] Train G Loss: 0.14426\t\n",
      "[23/200] Valiation G Loss: 0.19200\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.16169780492782593 / D Loss: 0.01472831703722477\n",
      "[100] G Loss: 0.14948686957359314 / D Loss: 0.03235957399010658\n",
      "-------------------------------------------------------------------\n",
      "[24/200] Train G Loss: 0.14453\t\n",
      "[24/200] Valiation G Loss: 0.20051\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1445871740579605 / D Loss: 0.061202503740787506\n",
      "[100] G Loss: 0.1455160528421402 / D Loss: 0.03668086230754852\n",
      "-------------------------------------------------------------------\n",
      "[25/200] Train G Loss: 0.14249\t\n",
      "<< Best model save at [25] epoch! >>\n",
      "<< model save at [25] epoch! >>\n",
      "[25/200] Valiation G Loss: 0.18725\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13016296923160553 / D Loss: 0.07883080095052719\n",
      "[100] G Loss: 0.13210438191890717 / D Loss: 0.10795217752456665\n",
      "-------------------------------------------------------------------\n",
      "[26/200] Train G Loss: 0.14153\t\n",
      "[26/200] Valiation G Loss: 0.19444\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12486115097999573 / D Loss: 0.27124500274658203\n",
      "[100] G Loss: 0.1508989930152893 / D Loss: 0.01247516367584467\n",
      "-------------------------------------------------------------------\n",
      "[27/200] Train G Loss: 0.14061\t\n",
      "<< Best model save at [27] epoch! >>\n",
      "[27/200] Valiation G Loss: 0.18624\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1401631385087967 / D Loss: 0.03077024221420288\n",
      "[100] G Loss: 0.15391454100608826 / D Loss: 0.01589989848434925\n",
      "-------------------------------------------------------------------\n",
      "[28/200] Train G Loss: 0.13924\t\n",
      "<< Best model save at [28] epoch! >>\n",
      "[28/200] Valiation G Loss: 0.18165\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12913617491722107 / D Loss: 0.056601762771606445\n",
      "[100] G Loss: 0.13677282631397247 / D Loss: 0.11990530788898468\n",
      "-------------------------------------------------------------------\n",
      "[29/200] Train G Loss: 0.14047\t\n",
      "[29/200] Valiation G Loss: 0.18431\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13533586263656616 / D Loss: 0.10912387818098068\n",
      "[100] G Loss: 0.11874086409807205 / D Loss: 0.3577544391155243\n",
      "-------------------------------------------------------------------\n",
      "[30/200] Train G Loss: 0.13584\t\n",
      "<< model save at [30] epoch! >>\n",
      "[30/200] Valiation G Loss: 0.18189\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13665823638439178 / D Loss: 0.0509452149271965\n",
      "[100] G Loss: 0.1435459405183792 / D Loss: 0.017846453934907913\n",
      "-------------------------------------------------------------------\n",
      "[31/200] Train G Loss: 0.13771\t\n",
      "[31/200] Valiation G Loss: 0.18283\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14310573041439056 / D Loss: 0.061959873884916306\n",
      "[100] G Loss: 0.1341092437505722 / D Loss: 0.11253289878368378\n",
      "-------------------------------------------------------------------\n",
      "[32/200] Train G Loss: 0.13934\t\n",
      "[32/200] Valiation G Loss: 0.18501\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1298474818468094 / D Loss: 0.08724379539489746\n",
      "[100] G Loss: 0.13744080066680908 / D Loss: 0.07294365763664246\n",
      "-------------------------------------------------------------------\n",
      "[33/200] Train G Loss: 0.13635\t\n",
      "<< Best model save at [33] epoch! >>\n",
      "[33/200] Valiation G Loss: 0.18013\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1317732036113739 / D Loss: 0.04971068352460861\n",
      "[100] G Loss: 0.13205499947071075 / D Loss: 0.027794241905212402\n",
      "-------------------------------------------------------------------\n",
      "[34/200] Train G Loss: 0.13538\t\n",
      "<< Best model save at [34] epoch! >>\n",
      "[34/200] Valiation G Loss: 0.17901\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1305595487356186 / D Loss: 0.1301477700471878\n",
      "[100] G Loss: 0.13393926620483398 / D Loss: 0.057824768126010895\n",
      "-------------------------------------------------------------------\n",
      "[35/200] Train G Loss: 0.13511\t\n",
      "<< Best model save at [35] epoch! >>\n",
      "<< model save at [35] epoch! >>\n",
      "[35/200] Valiation G Loss: 0.17819\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13179640471935272 / D Loss: 0.1978084295988083\n",
      "[100] G Loss: 0.13506998121738434 / D Loss: 0.04963114112615585\n",
      "-------------------------------------------------------------------\n",
      "[36/200] Train G Loss: 0.13676\t\n",
      "<< Best model save at [36] epoch! >>\n",
      "[36/200] Valiation G Loss: 0.17766\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15279649198055267 / D Loss: 0.031926680356264114\n",
      "[100] G Loss: 0.12953411042690277 / D Loss: 0.043894682079553604\n",
      "-------------------------------------------------------------------\n",
      "[37/200] Train G Loss: 0.13808\t\n",
      "[37/200] Valiation G Loss: 0.20859\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14822222292423248 / D Loss: 0.007742937654256821\n",
      "[100] G Loss: 0.13424484431743622 / D Loss: 0.016990575939416885\n",
      "-------------------------------------------------------------------\n",
      "[38/200] Train G Loss: 0.13887\t\n",
      "[38/200] Valiation G Loss: 0.18083\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13340266048908234 / D Loss: 0.03512536361813545\n",
      "[100] G Loss: 0.1335643231868744 / D Loss: 0.04938694089651108\n",
      "-------------------------------------------------------------------\n",
      "[39/200] Train G Loss: 0.13623\t\n",
      "[39/200] Valiation G Loss: 0.17797\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13496695458889008 / D Loss: 0.059227727353572845\n",
      "[100] G Loss: 0.149735227227211 / D Loss: 0.030885616317391396\n",
      "-------------------------------------------------------------------\n",
      "[40/200] Train G Loss: 0.13958\t\n",
      "<< model save at [40] epoch! >>\n",
      "[40/200] Valiation G Loss: 0.17938\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13498006761074066 / D Loss: 0.054909318685531616\n",
      "[100] G Loss: 0.1442146748304367 / D Loss: 0.004605914931744337\n",
      "-------------------------------------------------------------------\n",
      "[41/200] Train G Loss: 0.14163\t\n",
      "[41/200] Valiation G Loss: 0.18427\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14386220276355743 / D Loss: 0.012789605185389519\n",
      "[100] G Loss: 0.128974050283432 / D Loss: 0.06762763857841492\n",
      "-------------------------------------------------------------------\n",
      "[42/200] Train G Loss: 0.13900\t\n",
      "[42/200] Valiation G Loss: 0.17840\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14040344953536987 / D Loss: 0.02625281736254692\n",
      "[100] G Loss: 0.13318492472171783 / D Loss: 0.013191679492592812\n",
      "-------------------------------------------------------------------\n",
      "[43/200] Train G Loss: 0.13538\t\n",
      "[43/200] Valiation G Loss: 0.17811\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13988497853279114 / D Loss: 0.007050490006804466\n",
      "[100] G Loss: 0.12491527944803238 / D Loss: 0.02649063989520073\n",
      "-------------------------------------------------------------------\n",
      "[44/200] Train G Loss: 0.13465\t\n",
      "[44/200] Valiation G Loss: 0.18395\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1446947157382965 / D Loss: 0.01842048019170761\n",
      "[100] G Loss: 0.13306723535060883 / D Loss: 0.16716891527175903\n",
      "-------------------------------------------------------------------\n",
      "[45/200] Train G Loss: 0.13773\t\n",
      "<< model save at [45] epoch! >>\n",
      "[45/200] Valiation G Loss: 0.18850\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1525896191596985 / D Loss: 0.018029896542429924\n",
      "[100] G Loss: 0.1390368491411209 / D Loss: 0.030174653977155685\n",
      "-------------------------------------------------------------------\n",
      "[46/200] Train G Loss: 0.13770\t\n",
      "<< Best model save at [46] epoch! >>\n",
      "[46/200] Valiation G Loss: 0.17378\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13739518821239471 / D Loss: 0.0041662841103971004\n",
      "[100] G Loss: 0.12244629859924316 / D Loss: 0.07460427284240723\n",
      "-------------------------------------------------------------------\n",
      "[47/200] Train G Loss: 0.13618\t\n",
      "[47/200] Valiation G Loss: 0.17519\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.133436918258667 / D Loss: 0.06469619274139404\n",
      "[100] G Loss: 0.1322575956583023 / D Loss: 0.052601009607315063\n",
      "-------------------------------------------------------------------\n",
      "[48/200] Train G Loss: 0.13790\t\n",
      "[48/200] Valiation G Loss: 0.18303\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1378198266029358 / D Loss: 0.011674311943352222\n",
      "[100] G Loss: 0.13417889177799225 / D Loss: 0.0968518778681755\n",
      "-------------------------------------------------------------------\n",
      "[49/200] Train G Loss: 0.13664\t\n",
      "<< Best model save at [49] epoch! >>\n",
      "[49/200] Valiation G Loss: 0.17161\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13809078931808472 / D Loss: 0.040695443749427795\n",
      "[100] G Loss: 0.12932583689689636 / D Loss: 0.03242408111691475\n",
      "-------------------------------------------------------------------\n",
      "[50/200] Train G Loss: 0.13702\t\n",
      "<< model save at [50] epoch! >>\n",
      "[50/200] Valiation G Loss: 0.17551\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13838812708854675 / D Loss: 0.011426327750086784\n",
      "[100] G Loss: 0.14216625690460205 / D Loss: 0.050662457942962646\n",
      "-------------------------------------------------------------------\n",
      "[51/200] Train G Loss: 0.13401\t\n",
      "[51/200] Valiation G Loss: 0.17190\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15349704027175903 / D Loss: 0.0035141557455062866\n",
      "[100] G Loss: 0.13876259326934814 / D Loss: 0.01915643736720085\n",
      "-------------------------------------------------------------------\n",
      "[52/200] Train G Loss: 0.13913\t\n",
      "[52/200] Valiation G Loss: 0.17925\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14536096155643463 / D Loss: 0.026148641481995583\n",
      "[100] G Loss: 0.15006312727928162 / D Loss: 0.006805062294006348\n",
      "-------------------------------------------------------------------\n",
      "[53/200] Train G Loss: 0.13702\t\n",
      "[53/200] Valiation G Loss: 0.17320\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12568098306655884 / D Loss: 0.04761974513530731\n",
      "[100] G Loss: 0.13333182036876678 / D Loss: 0.029650256037712097\n",
      "-------------------------------------------------------------------\n",
      "[54/200] Train G Loss: 0.13330\t\n",
      "[54/200] Valiation G Loss: 0.17304\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1571996808052063 / D Loss: 0.02840818651020527\n",
      "[100] G Loss: 0.13191379606723785 / D Loss: 0.014415515586733818\n",
      "-------------------------------------------------------------------\n",
      "[55/200] Train G Loss: 0.13567\t\n",
      "<< model save at [55] epoch! >>\n",
      "[55/200] Valiation G Loss: 0.18831\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13685116171836853 / D Loss: 0.0030399898532778025\n",
      "[100] G Loss: 0.11474490910768509 / D Loss: 0.26311466097831726\n",
      "-------------------------------------------------------------------\n",
      "[56/200] Train G Loss: 0.13997\t\n",
      "[56/200] Valiation G Loss: 0.17284\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13677330315113068 / D Loss: 0.01873788610100746\n",
      "[100] G Loss: 0.12763351202011108 / D Loss: 0.045090869069099426\n",
      "-------------------------------------------------------------------\n",
      "[57/200] Train G Loss: 0.13330\t\n",
      "<< Best model save at [57] epoch! >>\n",
      "[57/200] Valiation G Loss: 0.16997\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14495521783828735 / D Loss: 0.008016003295779228\n",
      "[100] G Loss: 0.13401034474372864 / D Loss: 0.014370537362992764\n",
      "-------------------------------------------------------------------\n",
      "[58/200] Train G Loss: 0.13659\t\n",
      "[58/200] Valiation G Loss: 0.17432\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11472109705209732 / D Loss: 0.16133862733840942\n",
      "[100] G Loss: 0.12097145617008209 / D Loss: 0.06787776201963425\n",
      "-------------------------------------------------------------------\n",
      "[59/200] Train G Loss: 0.13388\t\n",
      "[59/200] Valiation G Loss: 0.17485\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13765546679496765 / D Loss: 0.0029825642704963684\n",
      "[100] G Loss: 0.11888716369867325 / D Loss: 0.09743627905845642\n",
      "-------------------------------------------------------------------\n",
      "[60/200] Train G Loss: 0.13805\t\n",
      "<< model save at [60] epoch! >>\n",
      "[60/200] Valiation G Loss: 0.17121\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13100120425224304 / D Loss: 0.04709857329726219\n",
      "[100] G Loss: 0.12992578744888306 / D Loss: 0.03084494173526764\n",
      "-------------------------------------------------------------------\n",
      "[61/200] Train G Loss: 0.13190\t\n",
      "[61/200] Valiation G Loss: 0.17352\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13716398179531097 / D Loss: 0.012101617641746998\n",
      "[100] G Loss: 0.1194574311375618 / D Loss: 0.08370744436979294\n",
      "-------------------------------------------------------------------\n",
      "[62/200] Train G Loss: 0.12946\t\n",
      "[62/200] Valiation G Loss: 0.17127\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15260785818099976 / D Loss: 0.023364195600152016\n",
      "[100] G Loss: 0.1504702866077423 / D Loss: 0.0016121679218485951\n",
      "-------------------------------------------------------------------\n",
      "[63/200] Train G Loss: 0.14168\t\n",
      "[63/200] Valiation G Loss: 0.17229\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1287698894739151 / D Loss: 0.06400969624519348\n",
      "[100] G Loss: 0.12758082151412964 / D Loss: 0.04542570933699608\n",
      "-------------------------------------------------------------------\n",
      "[64/200] Train G Loss: 0.13194\t\n",
      "[64/200] Valiation G Loss: 0.17098\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12965485453605652 / D Loss: 0.013658076524734497\n",
      "[100] G Loss: 0.12445659190416336 / D Loss: 0.05543418973684311\n",
      "-------------------------------------------------------------------\n",
      "[65/200] Train G Loss: 0.13320\t\n",
      "<< model save at [65] epoch! >>\n",
      "[65/200] Valiation G Loss: 0.19326\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1372770071029663 / D Loss: 0.029433630406856537\n",
      "[100] G Loss: 0.12971681356430054 / D Loss: 0.02688339166343212\n",
      "-------------------------------------------------------------------\n",
      "[66/200] Train G Loss: 0.13267\t\n",
      "[66/200] Valiation G Loss: 0.17054\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1350952535867691 / D Loss: 0.017881643027067184\n",
      "[100] G Loss: 0.1293213963508606 / D Loss: 0.011123457923531532\n",
      "-------------------------------------------------------------------\n",
      "[67/200] Train G Loss: 0.13153\t\n",
      "[67/200] Valiation G Loss: 0.17040\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13684576749801636 / D Loss: 0.030243434011936188\n",
      "[100] G Loss: 0.144266277551651 / D Loss: 0.04259607568383217\n",
      "-------------------------------------------------------------------\n",
      "[68/200] Train G Loss: 0.13317\t\n",
      "[68/200] Valiation G Loss: 0.17159\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13636448979377747 / D Loss: 0.007028490770608187\n",
      "[100] G Loss: 0.13501086831092834 / D Loss: 0.014699463732540607\n",
      "-------------------------------------------------------------------\n",
      "[69/200] Train G Loss: 0.13310\t\n",
      "[69/200] Valiation G Loss: 0.17853\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12432458996772766 / D Loss: 0.03911332041025162\n",
      "[100] G Loss: 0.14102870225906372 / D Loss: 0.007817278616130352\n",
      "-------------------------------------------------------------------\n",
      "[70/200] Train G Loss: 0.13252\t\n",
      "<< model save at [70] epoch! >>\n",
      "[70/200] Valiation G Loss: 0.17067\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13631528615951538 / D Loss: 0.007958047091960907\n",
      "[100] G Loss: 0.1300148069858551 / D Loss: 0.037600282579660416\n",
      "-------------------------------------------------------------------\n",
      "[71/200] Train G Loss: 0.13417\t\n",
      "[71/200] Valiation G Loss: 0.17107\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1341262310743332 / D Loss: 0.01628868095576763\n",
      "[100] G Loss: 0.1425871104001999 / D Loss: 0.004248969256877899\n",
      "-------------------------------------------------------------------\n",
      "[72/200] Train G Loss: 0.13109\t\n",
      "[72/200] Valiation G Loss: 0.17088\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13723284006118774 / D Loss: 0.010996387340128422\n",
      "[100] G Loss: 0.1196744441986084 / D Loss: 0.06292807310819626\n",
      "-------------------------------------------------------------------\n",
      "[73/200] Train G Loss: 0.13161\t\n",
      "<< Best model save at [73] epoch! >>\n",
      "[73/200] Valiation G Loss: 0.16911\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13088873028755188 / D Loss: 0.007312316447496414\n",
      "[100] G Loss: 0.12720279395580292 / D Loss: 0.04846569150686264\n",
      "-------------------------------------------------------------------\n",
      "[74/200] Train G Loss: 0.12933\t\n",
      "<< Best model save at [74] epoch! >>\n",
      "[74/200] Valiation G Loss: 0.16834\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13242022693157196 / D Loss: 0.07885999977588654\n",
      "[100] G Loss: 0.14750318229198456 / D Loss: 0.01058475486934185\n",
      "-------------------------------------------------------------------\n",
      "[75/200] Train G Loss: 0.13167\t\n",
      "<< model save at [75] epoch! >>\n",
      "[75/200] Valiation G Loss: 0.17524\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13559947907924652 / D Loss: 0.05550646036863327\n",
      "[100] G Loss: 0.13748802244663239 / D Loss: 0.00881189201027155\n",
      "-------------------------------------------------------------------\n",
      "[76/200] Train G Loss: 0.13619\t\n",
      "[76/200] Valiation G Loss: 0.17127\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14300097525119781 / D Loss: 0.004934155382215977\n",
      "[100] G Loss: 0.12927739322185516 / D Loss: 0.021236935630440712\n",
      "-------------------------------------------------------------------\n",
      "[77/200] Train G Loss: 0.13386\t\n",
      "<< Best model save at [77] epoch! >>\n",
      "[77/200] Valiation G Loss: 0.16753\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11330631375312805 / D Loss: 0.14481797814369202\n",
      "[100] G Loss: 0.12987513840198517 / D Loss: 0.0026904353871941566\n",
      "-------------------------------------------------------------------\n",
      "[78/200] Train G Loss: 0.13156\t\n",
      "[78/200] Valiation G Loss: 0.17104\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13905826210975647 / D Loss: 0.01414832565933466\n",
      "[100] G Loss: 0.125844344496727 / D Loss: 0.08074787259101868\n",
      "-------------------------------------------------------------------\n",
      "[79/200] Train G Loss: 0.13098\t\n",
      "[79/200] Valiation G Loss: 0.17017\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12203885614871979 / D Loss: 0.056867122650146484\n",
      "[100] G Loss: 0.1340348720550537 / D Loss: 0.016137544065713882\n",
      "-------------------------------------------------------------------\n",
      "[80/200] Train G Loss: 0.13150\t\n",
      "<< model save at [80] epoch! >>\n",
      "[80/200] Valiation G Loss: 0.17115\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13482189178466797 / D Loss: 0.011740442365407944\n",
      "[100] G Loss: 0.12148121744394302 / D Loss: 0.038201235234737396\n",
      "-------------------------------------------------------------------\n",
      "[81/200] Train G Loss: 0.13137\t\n",
      "[81/200] Valiation G Loss: 0.16923\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13004782795906067 / D Loss: 0.05341416224837303\n",
      "[100] G Loss: 0.12125192582607269 / D Loss: 0.10567094385623932\n",
      "-------------------------------------------------------------------\n",
      "[82/200] Train G Loss: 0.13208\t\n",
      "[82/200] Valiation G Loss: 0.16886\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13224777579307556 / D Loss: 0.00872344896197319\n",
      "[100] G Loss: 0.1308276206254959 / D Loss: 0.008420226164162159\n",
      "-------------------------------------------------------------------\n",
      "[83/200] Train G Loss: 0.13315\t\n",
      "[83/200] Valiation G Loss: 0.17075\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15151681005954742 / D Loss: 0.009320865385234356\n",
      "[100] G Loss: 0.11421321332454681 / D Loss: 0.1726205050945282\n",
      "-------------------------------------------------------------------\n",
      "[84/200] Train G Loss: 0.13136\t\n",
      "<< Best model save at [84] epoch! >>\n",
      "[84/200] Valiation G Loss: 0.16739\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11970524489879608 / D Loss: 0.06918910145759583\n",
      "[100] G Loss: 0.1291954666376114 / D Loss: 0.009067026898264885\n",
      "-------------------------------------------------------------------\n",
      "[85/200] Train G Loss: 0.13162\t\n",
      "<< model save at [85] epoch! >>\n",
      "[85/200] Valiation G Loss: 0.17118\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12601566314697266 / D Loss: 0.07199127227067947\n",
      "[100] G Loss: 0.135445237159729 / D Loss: 0.009763091802597046\n",
      "-------------------------------------------------------------------\n",
      "[86/200] Train G Loss: 0.13199\t\n",
      "[86/200] Valiation G Loss: 0.16777\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13609090447425842 / D Loss: 0.00209801341407001\n",
      "[100] G Loss: 0.13877719640731812 / D Loss: 0.0048103719018399715\n",
      "-------------------------------------------------------------------\n",
      "[87/200] Train G Loss: 0.13432\t\n",
      "[87/200] Valiation G Loss: 0.16872\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13850843906402588 / D Loss: 0.03669840097427368\n",
      "[100] G Loss: 0.14805468916893005 / D Loss: 0.018041666597127914\n",
      "-------------------------------------------------------------------\n",
      "[88/200] Train G Loss: 0.13169\t\n",
      "[88/200] Valiation G Loss: 0.16962\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14719919860363007 / D Loss: 0.004605564288794994\n",
      "[100] G Loss: 0.130081906914711 / D Loss: 0.01310341153293848\n",
      "-------------------------------------------------------------------\n",
      "[89/200] Train G Loss: 0.13407\t\n",
      "[89/200] Valiation G Loss: 0.17602\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13706159591674805 / D Loss: 0.00738768232986331\n",
      "[100] G Loss: 0.12341675907373428 / D Loss: 0.10955715924501419\n",
      "-------------------------------------------------------------------\n",
      "[90/200] Train G Loss: 0.13223\t\n",
      "<< model save at [90] epoch! >>\n",
      "[90/200] Valiation G Loss: 0.16795\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13798491656780243 / D Loss: 0.015822235494852066\n",
      "[100] G Loss: 0.1375667005777359 / D Loss: 0.0153602734208107\n",
      "-------------------------------------------------------------------\n",
      "[91/200] Train G Loss: 0.13311\t\n",
      "[91/200] Valiation G Loss: 0.17020\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13013000786304474 / D Loss: 0.01183752715587616\n",
      "[100] G Loss: 0.13115829229354858 / D Loss: 0.015578348189592361\n",
      "-------------------------------------------------------------------\n",
      "[92/200] Train G Loss: 0.13005\t\n",
      "[92/200] Valiation G Loss: 0.16973\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14024461805820465 / D Loss: 0.027941716834902763\n",
      "[100] G Loss: 0.12931716442108154 / D Loss: 0.015644770115613937\n",
      "-------------------------------------------------------------------\n",
      "[93/200] Train G Loss: 0.12870\t\n",
      "[93/200] Valiation G Loss: 0.16785\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.18441233038902283 / D Loss: 0.0063245403580367565\n",
      "[100] G Loss: 0.129536435008049 / D Loss: 0.0064500924199819565\n",
      "-------------------------------------------------------------------\n",
      "[94/200] Train G Loss: 0.13495\t\n",
      "[94/200] Valiation G Loss: 0.17612\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1281808465719223 / D Loss: 0.010073864832520485\n",
      "[100] G Loss: 0.12325845658779144 / D Loss: 0.07451902329921722\n",
      "-------------------------------------------------------------------\n",
      "[95/200] Train G Loss: 0.13024\t\n",
      "<< model save at [95] epoch! >>\n",
      "[95/200] Valiation G Loss: 0.16756\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11810075491666794 / D Loss: 0.054387167096138\n",
      "[100] G Loss: 0.12706372141838074 / D Loss: 0.021621696650981903\n",
      "-------------------------------------------------------------------\n",
      "[96/200] Train G Loss: 0.13259\t\n",
      "<< Best model save at [96] epoch! >>\n",
      "[96/200] Valiation G Loss: 0.16734\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13445958495140076 / D Loss: 0.0035449296701699495\n",
      "[100] G Loss: 0.13094551861286163 / D Loss: 0.01585552655160427\n",
      "-------------------------------------------------------------------\n",
      "[97/200] Train G Loss: 0.13077\t\n",
      "[97/200] Valiation G Loss: 0.16981\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13476069271564484 / D Loss: 0.004747563973069191\n",
      "[100] G Loss: 0.12928061187267303 / D Loss: 0.019362548366189003\n",
      "-------------------------------------------------------------------\n",
      "[98/200] Train G Loss: 0.13124\t\n",
      "[98/200] Valiation G Loss: 0.17272\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1346021592617035 / D Loss: 0.007536728400737047\n",
      "[100] G Loss: 0.13309261202812195 / D Loss: 0.006725749932229519\n",
      "-------------------------------------------------------------------\n",
      "[99/200] Train G Loss: 0.12755\t\n",
      "[99/200] Valiation G Loss: 0.16973\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12463750690221786 / D Loss: 0.04823702201247215\n",
      "[100] G Loss: 0.13341981172561646 / D Loss: 0.006696738302707672\n",
      "-------------------------------------------------------------------\n",
      "[100/200] Train G Loss: 0.13376\t\n",
      "<< model save at [100] epoch! >>\n",
      "[100/200] Valiation G Loss: 0.16872\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12622426450252533 / D Loss: 0.06077124923467636\n",
      "[100] G Loss: 0.15538842976093292 / D Loss: 0.004549707751721144\n",
      "-------------------------------------------------------------------\n",
      "[101/200] Train G Loss: 0.13535\t\n",
      "[101/200] Valiation G Loss: 0.17143\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12818633019924164 / D Loss: 0.011677485890686512\n",
      "[100] G Loss: 0.1311059147119522 / D Loss: 0.007917840033769608\n",
      "-------------------------------------------------------------------\n",
      "[102/200] Train G Loss: 0.13122\t\n",
      "[102/200] Valiation G Loss: 0.17158\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13868944346904755 / D Loss: 0.010209448635578156\n",
      "[100] G Loss: 0.13466167449951172 / D Loss: 0.006203777156770229\n",
      "-------------------------------------------------------------------\n",
      "[103/200] Train G Loss: 0.13811\t\n",
      "[103/200] Valiation G Loss: 0.17490\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13680987060070038 / D Loss: 0.007473878562450409\n",
      "[100] G Loss: 0.14087843894958496 / D Loss: 0.007420883979648352\n",
      "-------------------------------------------------------------------\n",
      "[104/200] Train G Loss: 0.13244\t\n",
      "[104/200] Valiation G Loss: 0.16927\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13436968624591827 / D Loss: 0.0028859039302915335\n",
      "[100] G Loss: 0.1320982426404953 / D Loss: 0.006780012510716915\n",
      "-------------------------------------------------------------------\n",
      "[105/200] Train G Loss: 0.13865\t\n",
      "<< model save at [105] epoch! >>\n",
      "[105/200] Valiation G Loss: 0.16854\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13512305915355682 / D Loss: 0.006673069205135107\n",
      "[100] G Loss: 0.13025684654712677 / D Loss: 0.003489676397293806\n",
      "-------------------------------------------------------------------\n",
      "[106/200] Train G Loss: 0.13325\t\n",
      "[106/200] Valiation G Loss: 0.17119\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11979501694440842 / D Loss: 0.10034382343292236\n",
      "[100] G Loss: 0.12863153219223022 / D Loss: 0.00673931697383523\n",
      "-------------------------------------------------------------------\n",
      "[107/200] Train G Loss: 0.13423\t\n",
      "[107/200] Valiation G Loss: 0.19013\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.132587730884552 / D Loss: 0.02133278176188469\n",
      "[100] G Loss: 0.14128419756889343 / D Loss: 0.012680795043706894\n",
      "-------------------------------------------------------------------\n",
      "[108/200] Train G Loss: 0.13305\t\n",
      "[108/200] Valiation G Loss: 0.16895\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12285885959863663 / D Loss: 0.04910307377576828\n",
      "[100] G Loss: 0.12069721519947052 / D Loss: 0.04854252189397812\n",
      "-------------------------------------------------------------------\n",
      "[109/200] Train G Loss: 0.12671\t\n",
      "[109/200] Valiation G Loss: 0.17851\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13594311475753784 / D Loss: 0.005672943312674761\n",
      "[100] G Loss: 0.12907549738883972 / D Loss: 0.017960675060749054\n",
      "-------------------------------------------------------------------\n",
      "[110/200] Train G Loss: 0.13272\t\n",
      "<< Best model save at [110] epoch! >>\n",
      "<< model save at [110] epoch! >>\n",
      "[110/200] Valiation G Loss: 0.16580\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1434459537267685 / D Loss: 0.0022177889477461576\n",
      "[100] G Loss: 0.12528972327709198 / D Loss: 0.005997845437377691\n",
      "-------------------------------------------------------------------\n",
      "[111/200] Train G Loss: 0.13150\t\n",
      "[111/200] Valiation G Loss: 0.16704\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11289061605930328 / D Loss: 0.12390580773353577\n",
      "[100] G Loss: 0.1374957412481308 / D Loss: 0.011932214722037315\n",
      "-------------------------------------------------------------------\n",
      "[112/200] Train G Loss: 0.12646\t\n",
      "[112/200] Valiation G Loss: 0.16960\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13350699841976166 / D Loss: 0.007982118055224419\n",
      "[100] G Loss: 0.10890387743711472 / D Loss: 0.320108026266098\n",
      "-------------------------------------------------------------------\n",
      "[113/200] Train G Loss: 0.12952\t\n",
      "[113/200] Valiation G Loss: 0.16702\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12465941905975342 / D Loss: 0.00518935127183795\n",
      "[100] G Loss: 0.14105235040187836 / D Loss: 0.02134614996612072\n",
      "-------------------------------------------------------------------\n",
      "[114/200] Train G Loss: 0.13199\t\n",
      "[114/200] Valiation G Loss: 0.17619\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1351754367351532 / D Loss: 0.005916345864534378\n",
      "[100] G Loss: 0.13735319674015045 / D Loss: 0.033703841269016266\n",
      "-------------------------------------------------------------------\n",
      "[115/200] Train G Loss: 0.13025\t\n",
      "<< model save at [115] epoch! >>\n",
      "[115/200] Valiation G Loss: 0.18778\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12362276017665863 / D Loss: 0.012423265725374222\n",
      "[100] G Loss: 0.13746874034404755 / D Loss: 0.005071414168924093\n",
      "-------------------------------------------------------------------\n",
      "[116/200] Train G Loss: 0.12926\t\n",
      "[116/200] Valiation G Loss: 0.17326\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1564323604106903 / D Loss: 0.01579822227358818\n",
      "[100] G Loss: 0.13149970769882202 / D Loss: 0.008103648200631142\n",
      "-------------------------------------------------------------------\n",
      "[117/200] Train G Loss: 0.13060\t\n",
      "<< Best model save at [117] epoch! >>\n",
      "[117/200] Valiation G Loss: 0.16407\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12544670701026917 / D Loss: 0.004848849959671497\n",
      "[100] G Loss: 0.1306747943162918 / D Loss: 0.061385173350572586\n",
      "-------------------------------------------------------------------\n",
      "[118/200] Train G Loss: 0.12614\t\n",
      "[118/200] Valiation G Loss: 0.17117\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12886421382427216 / D Loss: 0.00227168551646173\n",
      "[100] G Loss: 0.100713811814785 / D Loss: 0.34389060735702515\n",
      "-------------------------------------------------------------------\n",
      "[119/200] Train G Loss: 0.13094\t\n",
      "[119/200] Valiation G Loss: 0.16714\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1286069005727768 / D Loss: 0.037508875131607056\n",
      "[100] G Loss: 0.14146724343299866 / D Loss: 0.006029871292412281\n",
      "-------------------------------------------------------------------\n",
      "[120/200] Train G Loss: 0.12937\t\n",
      "<< model save at [120] epoch! >>\n",
      "[120/200] Valiation G Loss: 0.16914\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1375070959329605 / D Loss: 0.012046294286847115\n",
      "[100] G Loss: 0.13166576623916626 / D Loss: 0.0030527643393725157\n",
      "-------------------------------------------------------------------\n",
      "[121/200] Train G Loss: 0.12991\t\n",
      "[121/200] Valiation G Loss: 0.20001\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11831679940223694 / D Loss: 0.09151193499565125\n",
      "[100] G Loss: 0.13453644514083862 / D Loss: 0.0028958283364772797\n",
      "-------------------------------------------------------------------\n",
      "[122/200] Train G Loss: 0.13614\t\n",
      "[122/200] Valiation G Loss: 0.16680\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13290004432201385 / D Loss: 0.006504163611680269\n",
      "[100] G Loss: 0.13219350576400757 / D Loss: 0.012850558385252953\n",
      "-------------------------------------------------------------------\n",
      "[123/200] Train G Loss: 0.13135\t\n",
      "[123/200] Valiation G Loss: 0.16618\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1397951990365982 / D Loss: 0.002989674685522914\n",
      "[100] G Loss: 0.13256874680519104 / D Loss: 0.003157783765345812\n",
      "-------------------------------------------------------------------\n",
      "[124/200] Train G Loss: 0.13298\t\n",
      "[124/200] Valiation G Loss: 0.16907\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.15815269947052002 / D Loss: 0.004571070428937674\n",
      "[100] G Loss: 0.1504448503255844 / D Loss: 0.014080866239964962\n",
      "-------------------------------------------------------------------\n",
      "[125/200] Train G Loss: 0.13904\t\n",
      "<< model save at [125] epoch! >>\n",
      "[125/200] Valiation G Loss: 0.18430\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12045321613550186 / D Loss: 0.018263831734657288\n",
      "[100] G Loss: 0.1350991129875183 / D Loss: 0.003751749638468027\n",
      "-------------------------------------------------------------------\n",
      "[126/200] Train G Loss: 0.13041\t\n",
      "[126/200] Valiation G Loss: 0.16747\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.16076546907424927 / D Loss: 0.001177827944047749\n",
      "[100] G Loss: 0.14014442265033722 / D Loss: 0.0023086736910045147\n",
      "-------------------------------------------------------------------\n",
      "[127/200] Train G Loss: 0.13404\t\n",
      "[127/200] Valiation G Loss: 0.16738\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11722724139690399 / D Loss: 0.012052727863192558\n",
      "[100] G Loss: 0.11409787088632584 / D Loss: 0.12189304828643799\n",
      "-------------------------------------------------------------------\n",
      "[128/200] Train G Loss: 0.12987\t\n",
      "[128/200] Valiation G Loss: 0.16698\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12518328428268433 / D Loss: 0.023640912026166916\n",
      "[100] G Loss: 0.1311952769756317 / D Loss: 0.013552546501159668\n",
      "-------------------------------------------------------------------\n",
      "[129/200] Train G Loss: 0.12808\t\n",
      "<< Best model save at [129] epoch! >>\n",
      "[129/200] Valiation G Loss: 0.16245\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1187208816409111 / D Loss: 0.1523168981075287\n",
      "[100] G Loss: 0.12998424470424652 / D Loss: 0.0053511736914515495\n",
      "-------------------------------------------------------------------\n",
      "[130/200] Train G Loss: 0.12752\t\n",
      "<< model save at [130] epoch! >>\n",
      "[130/200] Valiation G Loss: 0.16726\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13004516065120697 / D Loss: 0.003762980457395315\n",
      "[100] G Loss: 0.10510227084159851 / D Loss: 0.18898054957389832\n",
      "-------------------------------------------------------------------\n",
      "[131/200] Train G Loss: 0.12908\t\n",
      "[131/200] Valiation G Loss: 0.18891\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12454502284526825 / D Loss: 0.023718178272247314\n",
      "[100] G Loss: 0.12736882269382477 / D Loss: 0.0067158909514546394\n",
      "-------------------------------------------------------------------\n",
      "[132/200] Train G Loss: 0.12435\t\n",
      "[132/200] Valiation G Loss: 0.17851\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13276834785938263 / D Loss: 0.009099682793021202\n",
      "[100] G Loss: 0.12992051243782043 / D Loss: 0.005926342215389013\n",
      "-------------------------------------------------------------------\n",
      "[133/200] Train G Loss: 0.12966\t\n",
      "[133/200] Valiation G Loss: 0.16764\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14887602627277374 / D Loss: 0.011330757290124893\n",
      "[100] G Loss: 0.12980668246746063 / D Loss: 0.0030270619317889214\n",
      "-------------------------------------------------------------------\n",
      "[134/200] Train G Loss: 0.12841\t\n",
      "[134/200] Valiation G Loss: 0.16932\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1294620782136917 / D Loss: 0.021829957142472267\n",
      "[100] G Loss: 0.12343759834766388 / D Loss: 0.003566698171198368\n",
      "-------------------------------------------------------------------\n",
      "[135/200] Train G Loss: 0.12842\t\n",
      "<< model save at [135] epoch! >>\n",
      "[135/200] Valiation G Loss: 0.16852\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12197406589984894 / D Loss: 0.053157955408096313\n",
      "[100] G Loss: 0.13652069866657257 / D Loss: 0.0105963833630085\n",
      "-------------------------------------------------------------------\n",
      "[136/200] Train G Loss: 0.12920\t\n",
      "[136/200] Valiation G Loss: 0.17571\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12650078535079956 / D Loss: 0.0015491624362766743\n",
      "[100] G Loss: 0.12672442197799683 / D Loss: 0.039387911558151245\n",
      "-------------------------------------------------------------------\n",
      "[137/200] Train G Loss: 0.12617\t\n",
      "[137/200] Valiation G Loss: 0.16710\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12890635430812836 / D Loss: 0.0016048656543716788\n",
      "[100] G Loss: 0.12462802976369858 / D Loss: 0.04799380153417587\n",
      "-------------------------------------------------------------------\n",
      "[138/200] Train G Loss: 0.12962\t\n",
      "[138/200] Valiation G Loss: 0.16632\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14281056821346283 / D Loss: 0.01999276503920555\n",
      "[100] G Loss: 0.12009566277265549 / D Loss: 0.10719013959169388\n",
      "-------------------------------------------------------------------\n",
      "[139/200] Train G Loss: 0.12724\t\n",
      "[139/200] Valiation G Loss: 0.16798\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14039337635040283 / D Loss: 0.00472966767847538\n",
      "[100] G Loss: 0.13615523278713226 / D Loss: 0.004603630863130093\n",
      "-------------------------------------------------------------------\n",
      "[140/200] Train G Loss: 0.12813\t\n",
      "<< model save at [140] epoch! >>\n",
      "[140/200] Valiation G Loss: 0.17573\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13196797668933868 / D Loss: 0.010166807100176811\n",
      "[100] G Loss: 0.1379406899213791 / D Loss: 0.011207600124180317\n",
      "-------------------------------------------------------------------\n",
      "[141/200] Train G Loss: 0.13091\t\n",
      "[141/200] Valiation G Loss: 0.17225\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12102343142032623 / D Loss: 0.05831370875239372\n",
      "[100] G Loss: 0.1369706243276596 / D Loss: 0.004262161441147327\n",
      "-------------------------------------------------------------------\n",
      "[142/200] Train G Loss: 0.12675\t\n",
      "[142/200] Valiation G Loss: 0.19358\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1373850554227829 / D Loss: 0.013339639641344547\n",
      "[100] G Loss: 0.12331867218017578 / D Loss: 0.011958470568060875\n",
      "-------------------------------------------------------------------\n",
      "[143/200] Train G Loss: 0.12425\t\n",
      "[143/200] Valiation G Loss: 0.16979\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14810138940811157 / D Loss: 0.029502442106604576\n",
      "[100] G Loss: 0.1228923499584198 / D Loss: 0.04242671653628349\n",
      "-------------------------------------------------------------------\n",
      "[144/200] Train G Loss: 0.12655\t\n",
      "[144/200] Valiation G Loss: 0.17047\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13815858960151672 / D Loss: 0.003992378246039152\n",
      "[100] G Loss: 0.12786804139614105 / D Loss: 0.004754979163408279\n",
      "-------------------------------------------------------------------\n",
      "[145/200] Train G Loss: 0.13247\t\n",
      "<< model save at [145] epoch! >>\n",
      "[145/200] Valiation G Loss: 0.16716\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13939039409160614 / D Loss: 0.0053129191510379314\n",
      "[100] G Loss: 0.1241830438375473 / D Loss: 0.002241811016574502\n",
      "-------------------------------------------------------------------\n",
      "[146/200] Train G Loss: 0.13182\t\n",
      "[146/200] Valiation G Loss: 0.16667\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12054148316383362 / D Loss: 0.054167699068784714\n",
      "[100] G Loss: 0.12327319383621216 / D Loss: 0.020311851054430008\n",
      "-------------------------------------------------------------------\n",
      "[147/200] Train G Loss: 0.12511\t\n",
      "[147/200] Valiation G Loss: 0.17830\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1179427057504654 / D Loss: 0.05077745020389557\n",
      "[100] G Loss: 0.12751469016075134 / D Loss: 0.003378927009180188\n",
      "-------------------------------------------------------------------\n",
      "[148/200] Train G Loss: 0.13128\t\n",
      "[148/200] Valiation G Loss: 0.18029\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13141725957393646 / D Loss: 0.002992099616676569\n",
      "[100] G Loss: 0.13045163452625275 / D Loss: 0.0022895822767168283\n",
      "-------------------------------------------------------------------\n",
      "[149/200] Train G Loss: 0.13090\t\n",
      "[149/200] Valiation G Loss: 0.16443\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12726594507694244 / D Loss: 0.008181425742805004\n",
      "[100] G Loss: 0.13628827035427094 / D Loss: 0.0020746299996972084\n",
      "-------------------------------------------------------------------\n",
      "[150/200] Train G Loss: 0.13355\t\n",
      "<< model save at [150] epoch! >>\n",
      "[150/200] Valiation G Loss: 0.17314\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14329762756824493 / D Loss: 0.00627550994977355\n",
      "[100] G Loss: 0.12240056693553925 / D Loss: 0.09303519129753113\n",
      "-------------------------------------------------------------------\n",
      "[151/200] Train G Loss: 0.12626\t\n",
      "[151/200] Valiation G Loss: 0.16500\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1413579285144806 / D Loss: 0.0038568146992474794\n",
      "[100] G Loss: 0.13460715115070343 / D Loss: 0.00350861600600183\n",
      "-------------------------------------------------------------------\n",
      "[152/200] Train G Loss: 0.13215\t\n",
      "<< Best model save at [152] epoch! >>\n",
      "[152/200] Valiation G Loss: 0.16174\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14390124380588531 / D Loss: 0.0024655035231262445\n",
      "[100] G Loss: 0.12453574687242508 / D Loss: 0.03885245323181152\n",
      "-------------------------------------------------------------------\n",
      "[153/200] Train G Loss: 0.13097\t\n",
      "[153/200] Valiation G Loss: 0.16696\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.16452986001968384 / D Loss: 0.02762746252119541\n",
      "[100] G Loss: 0.12295547127723694 / D Loss: 0.037948522716760635\n",
      "-------------------------------------------------------------------\n",
      "[154/200] Train G Loss: 0.13051\t\n",
      "[154/200] Valiation G Loss: 0.19888\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1386822611093521 / D Loss: 0.010443195700645447\n",
      "[100] G Loss: 0.1271798312664032 / D Loss: 0.012246103957295418\n",
      "-------------------------------------------------------------------\n",
      "[155/200] Train G Loss: 0.12771\t\n",
      "<< model save at [155] epoch! >>\n",
      "[155/200] Valiation G Loss: 0.16770\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13638751208782196 / D Loss: 0.009971711784601212\n",
      "[100] G Loss: 0.12492731213569641 / D Loss: 0.0067948633804917336\n",
      "-------------------------------------------------------------------\n",
      "[156/200] Train G Loss: 0.12586\t\n",
      "[156/200] Valiation G Loss: 0.16887\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1246308907866478 / D Loss: 0.020702719688415527\n",
      "[100] G Loss: 0.13173753023147583 / D Loss: 0.0036089178174734116\n",
      "-------------------------------------------------------------------\n",
      "[157/200] Train G Loss: 0.12698\t\n",
      "[157/200] Valiation G Loss: 0.16680\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11751765012741089 / D Loss: 0.046900998800992966\n",
      "[100] G Loss: 0.13653357326984406 / D Loss: 0.016560811549425125\n",
      "-------------------------------------------------------------------\n",
      "[158/200] Train G Loss: 0.12756\t\n",
      "[158/200] Valiation G Loss: 0.17564\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1294369399547577 / D Loss: 0.002757224952802062\n",
      "[100] G Loss: 0.13688015937805176 / D Loss: 0.00575113482773304\n",
      "-------------------------------------------------------------------\n",
      "[159/200] Train G Loss: 0.12627\t\n",
      "[159/200] Valiation G Loss: 0.16831\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12304356694221497 / D Loss: 0.029217571020126343\n",
      "[100] G Loss: 0.11964277178049088 / D Loss: 0.02192426286637783\n",
      "-------------------------------------------------------------------\n",
      "[160/200] Train G Loss: 0.12210\t\n",
      "<< model save at [160] epoch! >>\n",
      "[160/200] Valiation G Loss: 0.16708\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12450871616601944 / D Loss: 0.0033467754255980253\n",
      "[100] G Loss: 0.14040665328502655 / D Loss: 0.0026889913715422153\n",
      "-------------------------------------------------------------------\n",
      "[161/200] Train G Loss: 0.13188\t\n",
      "[161/200] Valiation G Loss: 0.17072\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13165925443172455 / D Loss: 0.0022800981532782316\n",
      "[100] G Loss: 0.11831735074520111 / D Loss: 0.13871321082115173\n",
      "-------------------------------------------------------------------\n",
      "[162/200] Train G Loss: 0.12799\t\n",
      "[162/200] Valiation G Loss: 0.17688\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1317005753517151 / D Loss: 0.022637033835053444\n",
      "[100] G Loss: 0.1545514017343521 / D Loss: 0.0006717474316246808\n",
      "-------------------------------------------------------------------\n",
      "[163/200] Train G Loss: 0.13250\t\n",
      "[163/200] Valiation G Loss: 0.18920\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11572548747062683 / D Loss: 0.007336590904742479\n",
      "[100] G Loss: 0.1391436606645584 / D Loss: 0.0018432963406667113\n",
      "-------------------------------------------------------------------\n",
      "[164/200] Train G Loss: 0.12792\t\n",
      "[164/200] Valiation G Loss: 0.18189\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12294923514127731 / D Loss: 0.0025684142019599676\n",
      "[100] G Loss: 0.09889180958271027 / D Loss: 0.4396207630634308\n",
      "-------------------------------------------------------------------\n",
      "[165/200] Train G Loss: 0.12860\t\n",
      "<< model save at [165] epoch! >>\n",
      "[165/200] Valiation G Loss: 0.16625\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1244078129529953 / D Loss: 0.002534248400479555\n",
      "[100] G Loss: 0.1195048987865448 / D Loss: 0.04171424359083176\n",
      "-------------------------------------------------------------------\n",
      "[166/200] Train G Loss: 0.12844\t\n",
      "[166/200] Valiation G Loss: 0.16712\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12317539006471634 / D Loss: 0.0016005339566618204\n",
      "[100] G Loss: 0.12057988345623016 / D Loss: 0.03296402841806412\n",
      "-------------------------------------------------------------------\n",
      "[167/200] Train G Loss: 0.12569\t\n",
      "[167/200] Valiation G Loss: 0.16870\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1367790699005127 / D Loss: 0.0049491990357637405\n",
      "[100] G Loss: 0.10924561321735382 / D Loss: 0.10277409106492996\n",
      "-------------------------------------------------------------------\n",
      "[168/200] Train G Loss: 0.13448\t\n",
      "[168/200] Valiation G Loss: 0.18827\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12088766694068909 / D Loss: 0.029249677434563637\n",
      "[100] G Loss: 0.14362451434135437 / D Loss: 0.0071697053499519825\n",
      "-------------------------------------------------------------------\n",
      "[169/200] Train G Loss: 0.12625\t\n",
      "[169/200] Valiation G Loss: 0.16712\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12123113125562668 / D Loss: 0.01171822939068079\n",
      "[100] G Loss: 0.13092267513275146 / D Loss: 0.0036533146630972624\n",
      "-------------------------------------------------------------------\n",
      "[170/200] Train G Loss: 0.13039\t\n",
      "<< model save at [170] epoch! >>\n",
      "[170/200] Valiation G Loss: 0.17679\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11989472061395645 / D Loss: 0.007626218255609274\n",
      "[100] G Loss: 0.1225489005446434 / D Loss: 0.023562896996736526\n",
      "-------------------------------------------------------------------\n",
      "[171/200] Train G Loss: 0.12939\t\n",
      "[171/200] Valiation G Loss: 0.16718\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12950082123279572 / D Loss: 0.02044050209224224\n",
      "[100] G Loss: 0.12955307960510254 / D Loss: 0.005415411200374365\n",
      "-------------------------------------------------------------------\n",
      "[172/200] Train G Loss: 0.12256\t\n",
      "[172/200] Valiation G Loss: 0.17443\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1279839426279068 / D Loss: 0.0031342124566435814\n",
      "[100] G Loss: 0.1317671239376068 / D Loss: 0.014149891212582588\n",
      "-------------------------------------------------------------------\n",
      "[173/200] Train G Loss: 0.12876\t\n",
      "[173/200] Valiation G Loss: 0.16764\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12121935188770294 / D Loss: 0.17236977815628052\n",
      "[100] G Loss: 0.12219993025064468 / D Loss: 0.0031910783145576715\n",
      "-------------------------------------------------------------------\n",
      "[174/200] Train G Loss: 0.13246\t\n",
      "[174/200] Valiation G Loss: 0.17038\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10143203288316727 / D Loss: 0.341932475566864\n",
      "[100] G Loss: 0.12558972835540771 / D Loss: 0.003389307763427496\n",
      "-------------------------------------------------------------------\n",
      "[175/200] Train G Loss: 0.12268\t\n",
      "<< model save at [175] epoch! >>\n",
      "[175/200] Valiation G Loss: 0.17352\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12902694940567017 / D Loss: 0.001690591685473919\n",
      "[100] G Loss: 0.1285649985074997 / D Loss: 0.04770256578922272\n",
      "-------------------------------------------------------------------\n",
      "[176/200] Train G Loss: 0.12765\t\n",
      "[176/200] Valiation G Loss: 0.16691\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1364082545042038 / D Loss: 0.03944779932498932\n",
      "[100] G Loss: 0.11375133693218231 / D Loss: 0.028821252286434174\n",
      "-------------------------------------------------------------------\n",
      "[177/200] Train G Loss: 0.12598\t\n",
      "[177/200] Valiation G Loss: 0.17958\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12431007623672485 / D Loss: 0.010439623147249222\n",
      "[100] G Loss: 0.15745557844638824 / D Loss: 0.011776399798691273\n",
      "-------------------------------------------------------------------\n",
      "[178/200] Train G Loss: 0.13049\t\n",
      "[178/200] Valiation G Loss: 0.19497\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12786062061786652 / D Loss: 0.04213538020849228\n",
      "[100] G Loss: 0.1317821443080902 / D Loss: 0.006439297925680876\n",
      "-------------------------------------------------------------------\n",
      "[179/200] Train G Loss: 0.12906\t\n",
      "[179/200] Valiation G Loss: 0.18107\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13291522860527039 / D Loss: 0.00602427963167429\n",
      "[100] G Loss: 0.1303952932357788 / D Loss: 0.001559379743412137\n",
      "-------------------------------------------------------------------\n",
      "[180/200] Train G Loss: 0.12904\t\n",
      "<< model save at [180] epoch! >>\n",
      "[180/200] Valiation G Loss: 0.16667\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1251385360956192 / D Loss: 0.0033925818279385567\n",
      "[100] G Loss: 0.12055297195911407 / D Loss: 0.00422967504709959\n",
      "-------------------------------------------------------------------\n",
      "[181/200] Train G Loss: 0.12852\t\n",
      "[181/200] Valiation G Loss: 0.17047\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12366320937871933 / D Loss: 0.05666910111904144\n",
      "[100] G Loss: 0.12051361799240112 / D Loss: 0.01296963356435299\n",
      "-------------------------------------------------------------------\n",
      "[182/200] Train G Loss: 0.12780\t\n",
      "[182/200] Valiation G Loss: 0.17385\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13241727650165558 / D Loss: 0.0067503140307962894\n",
      "[100] G Loss: 0.11298434436321259 / D Loss: 0.07443755865097046\n",
      "-------------------------------------------------------------------\n",
      "[183/200] Train G Loss: 0.12479\t\n",
      "[183/200] Valiation G Loss: 0.17684\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12057831138372421 / D Loss: 0.1307619959115982\n",
      "[100] G Loss: 0.11779054999351501 / D Loss: 0.031162980943918228\n",
      "-------------------------------------------------------------------\n",
      "[184/200] Train G Loss: 0.12334\t\n",
      "[184/200] Valiation G Loss: 0.17516\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1307373344898224 / D Loss: 0.014623712748289108\n",
      "[100] G Loss: 0.11853592097759247 / D Loss: 0.08076118677854538\n",
      "-------------------------------------------------------------------\n",
      "[185/200] Train G Loss: 0.13043\t\n",
      "<< model save at [185] epoch! >>\n",
      "[185/200] Valiation G Loss: 0.18627\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11023586243391037 / D Loss: 0.061580173671245575\n",
      "[100] G Loss: 0.14205513894557953 / D Loss: 0.00474811065942049\n",
      "-------------------------------------------------------------------\n",
      "[186/200] Train G Loss: 0.12710\t\n",
      "[186/200] Valiation G Loss: 0.18164\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12073894590139389 / D Loss: 0.0076606422662734985\n",
      "[100] G Loss: 0.12702669203281403 / D Loss: 0.007999081164598465\n",
      "-------------------------------------------------------------------\n",
      "[187/200] Train G Loss: 0.12416\t\n",
      "[187/200] Valiation G Loss: 0.16829\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13253198564052582 / D Loss: 0.003175221849232912\n",
      "[100] G Loss: 0.13375407457351685 / D Loss: 0.007501303683966398\n",
      "-------------------------------------------------------------------\n",
      "[188/200] Train G Loss: 0.12610\t\n",
      "[188/200] Valiation G Loss: 0.17626\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11942960321903229 / D Loss: 0.0021908185444772243\n",
      "[100] G Loss: 0.12363670021295547 / D Loss: 0.003238358534872532\n",
      "-------------------------------------------------------------------\n",
      "[189/200] Train G Loss: 0.12756\t\n",
      "[189/200] Valiation G Loss: 0.16377\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12239014357328415 / D Loss: 0.0025834408588707447\n",
      "[100] G Loss: 0.13309425115585327 / D Loss: 0.002156634349375963\n",
      "-------------------------------------------------------------------\n",
      "[190/200] Train G Loss: 0.12818\t\n",
      "<< model save at [190] epoch! >>\n",
      "[190/200] Valiation G Loss: 0.16395\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13707314431667328 / D Loss: 0.0068611688911914825\n",
      "[100] G Loss: 0.1473226547241211 / D Loss: 0.0071774558164179325\n",
      "-------------------------------------------------------------------\n",
      "[191/200] Train G Loss: 0.12792\t\n",
      "[191/200] Valiation G Loss: 0.16293\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12685337662696838 / D Loss: 0.004551468417048454\n",
      "[100] G Loss: 0.11788475513458252 / D Loss: 0.03490455076098442\n",
      "-------------------------------------------------------------------\n",
      "[192/200] Train G Loss: 0.12719\t\n",
      "[192/200] Valiation G Loss: 0.17383\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11642120033502579 / D Loss: 0.051989756524562836\n",
      "[100] G Loss: 0.12904132902622223 / D Loss: 0.002473067957907915\n",
      "-------------------------------------------------------------------\n",
      "[193/200] Train G Loss: 0.12770\t\n",
      "[193/200] Valiation G Loss: 0.16696\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1013168916106224 / D Loss: 0.40842747688293457\n",
      "[100] G Loss: 0.12137912213802338 / D Loss: 0.013833041302859783\n",
      "-------------------------------------------------------------------\n",
      "[194/200] Train G Loss: 0.12839\t\n",
      "[194/200] Valiation G Loss: 0.16444\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13196733593940735 / D Loss: 0.018208589404821396\n",
      "[100] G Loss: 0.13385653495788574 / D Loss: 0.004392443690448999\n",
      "-------------------------------------------------------------------\n",
      "[195/200] Train G Loss: 0.12854\t\n",
      "<< model save at [195] epoch! >>\n",
      "[195/200] Valiation G Loss: 0.17443\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12440785020589828 / D Loss: 0.0026351644191890955\n",
      "[100] G Loss: 0.1204758733510971 / D Loss: 0.017591530457139015\n",
      "-------------------------------------------------------------------\n",
      "[196/200] Train G Loss: 0.12756\t\n",
      "[196/200] Valiation G Loss: 0.17899\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.133774071931839 / D Loss: 0.006267980206757784\n",
      "[100] G Loss: 0.12779000401496887 / D Loss: 0.004978896584361792\n",
      "-------------------------------------------------------------------\n",
      "[197/200] Train G Loss: 0.12315\t\n",
      "[197/200] Valiation G Loss: 0.16465\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10208489000797272 / D Loss: 0.18565872311592102\n",
      "[100] G Loss: 0.11166371405124664 / D Loss: 0.09317002445459366\n",
      "-------------------------------------------------------------------\n",
      "[198/200] Train G Loss: 0.12779\t\n",
      "[198/200] Valiation G Loss: 0.16366\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1195555031299591 / D Loss: 0.0519871786236763\n",
      "[100] G Loss: 0.14349627494812012 / D Loss: 0.01642639748752117\n",
      "-------------------------------------------------------------------\n",
      "[199/200] Train G Loss: 0.13696\t\n",
      "[199/200] Valiation G Loss: 0.18326\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14845925569534302 / D Loss: 0.019821813330054283\n",
      "[100] G Loss: 0.18540364503860474 / D Loss: 0.012826407328248024\n",
      "-------------------------------------------------------------------\n",
      "[200/200] Train G Loss: 0.13563\t\n",
      "<< model save at [200] epoch! >>\n",
      "[200/200] Valiation G Loss: 0.17470\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "<< Finished Training >>\n",
      "<< Last Model Saved >>\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(epochs):\n",
    "    train_sum_loss = 0\n",
    "    val_sum_loss = 0  \n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        nd, qd = data\n",
    " \n",
    "        input = qd.to(device) # qd = input\n",
    "\n",
    "        target1 = (nd).to(device) # nd = target1\n",
    "        target2 = (nd-qd).to(device) # difference map = target2\n",
    "        target = torch.cat([target1, target2], dim=1)\n",
    " \n",
    "        # train G\n",
    "        out_g = generator(input) # predict nd \n",
    "        sub_g = out_g - input # predict diff map \n",
    "        pred = torch.cat([out_g, sub_g], dim=1).to(device)\n",
    "\n",
    "        fake = discriminator(pred) # Fake [pred nd, pred diff] <for adv learning> \n",
    "        loss_gen = getG_loss(out=out_g, sub=sub_g, out_target=target1, sub_target=target2, fake=fake, coefs=coefs)\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_g.step()\n",
    "        train_sum_loss += loss_gen.item()\n",
    "\n",
    "        # train D\n",
    "        out_d1 = discriminator(target) # Real [nd, diff]\n",
    "        out_d2 = discriminator(pred.detach()) # Fake [pred nd, pred diff]\n",
    "\n",
    "        loss_dis = getD_loss(real=out_d1, fake=out_d2)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        loss_dis.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # check loss per 50 iteration\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"[{i+1}] G Loss: {loss_gen.item()} / D Loss: {loss_dis.item()}\")\n",
    "\n",
    "    # record train loss\n",
    "    tr_loss = train_sum_loss / len(train_loader)\n",
    "    train_g_losses.append(tr_loss)\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('[%d/%d] Train G Loss: %.5f\\t'% (epoch+1, epochs, tr_loss))\n",
    "\n",
    "    # validation per one epoch\n",
    "    for j, data_v in enumerate(valid_loader, 0):\n",
    "        nd_v, qd_v = data_v\n",
    "\n",
    "        input_v = qd_v.to(device) # qd_v = input_v\n",
    "\n",
    "        target1_v = (nd_v).to(device) # nd_v = target1_v\n",
    "        target2_v = (nd_v-qd_v).to(device) # difference map = target2_v\n",
    "\n",
    "        out_v = generator(input_v) # predict nd_v\n",
    "        sub_v = out_v - input_v # predict diff map\n",
    "        pred_v = torch.cat([out_v, sub_v], dim=1)\n",
    "        fake_v = discriminator(pred_v) # Fake [pred nd, pred diff] <for adv learning> \n",
    "\n",
    "        loss_v = getG_loss(out=out_v, sub=sub_v, out_target=target1_v, sub_target=target2_v, fake=fake_v, coefs=coefs)\n",
    "        val_sum_loss += loss_v.item()\n",
    "\n",
    "    # save model\n",
    "    val_loss = val_sum_loss / len(valid_loader)\n",
    "    if (epoch+1) >= 10:\n",
    "        # early stopping\n",
    "        if val_loss < min(valid_g_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(generator.state_dict(), BEST_SAVE_PATH)\n",
    "            print('<< Best model save at [%d] epoch! >>' % (epoch+1))\n",
    "        # save per 5 epoch\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(generator.state_dict(), SAVE_PATH+f\"_{epoch+1}.pth\")\n",
    "            print('<< model save at [%d] epoch! >>' % (epoch+1))\n",
    "\n",
    "    # record validation loss\n",
    "    valid_g_losses.append(val_loss)\n",
    "    print('[%d/%d] Valiation G Loss: %.5f\\t'% (epoch+1, epochs, val_loss))\n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "\n",
    "print('<< Finished Training >>')\n",
    "torch.save(generator.state_dict(), LAST_SAVE_PATH)\n",
    "print(\"<< Last Model Saved >>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Denoising (AttnGAN)\")\n",
    "plt.plot(train_g_losses,label=\"train loss\")\n",
    "plt.plot(valid_g_losses,label=\"val loss\")\n",
    "plt.axvline(best_epoch, color='green', linestyle='--', linewidth=2, label=f\"early stopping ({best_epoch})\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
