{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model.generator import UNet\n",
    "from model.discriminator import PixelDiscriminator\n",
    "from pytorch_model_summary import summary\n",
    "from dataset import *\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0480) tensor(3.7225)\n",
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_path = 'data/trainset.npy'\n",
    "valid_path = 'data/validset.npy'\n",
    "\n",
    "train_set = npyDataset(npy_file=train_path, resize_h=256, resize_w=256)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_set = npyDataset(npy_file=valid_path, resize_h=256, resize_w=256)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataiter = iter(train_set)\n",
    "targets, inputs  = next(dataiter)\n",
    "print(torch.min(targets), torch.max(targets))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 256\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "\n",
    "def initialize_weights(m):\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "      if m.bias is not None:\n",
    "          nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.BatchNorm2d):\n",
    "      nn.init.constant_(m.weight.data, 1)\n",
    "      nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.Linear):\n",
    "      nn.init.kaiming_uniform_(m.weight.data)\n",
    "      nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          inconv-1     [16, 32, 256, 256]           9,632           9,632\n",
      "            down-2     [16, 64, 128, 128]          55,552          55,552\n",
      "            down-3      [16, 128, 64, 64]         221,696         221,696\n",
      "            down-4      [16, 256, 32, 32]         885,760         885,760\n",
      "            down-5      [16, 512, 16, 16]       3,540,992       3,540,992\n",
      "              up-6      [16, 256, 32, 32]       2,295,040       2,295,040\n",
      "          Conv2d-7        [16, 1, 32, 32]              65              65\n",
      "              up-8      [16, 128, 64, 64]         574,080         574,080\n",
      "          Conv2d-9        [16, 1, 64, 64]              17              17\n",
      "             up-10     [16, 64, 128, 128]         143,680         143,680\n",
      "         Conv2d-11      [16, 1, 128, 128]               5               5\n",
      "             up-12     [16, 32, 256, 256]          36,000          36,000\n",
      "         Conv2d-13      [16, 1, 256, 256]             289             289\n",
      "==========================================================================\n",
      "Total params: 7,762,808\n",
      "Trainable params: 7,762,808\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([16, 1, 256, 256])\n",
      "output: torch.Size([16, 1, 256, 256])\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): inconv(\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down1): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): up(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): up(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): up(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): up(\n",
       "    (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (size_down8): Conv2d(1, 1, kernel_size=(8, 8), stride=(8, 8))\n",
       "  (size_down4): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (size_down2): Conv2d(1, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = UNet(in_channels, out_channels).to(device)\n",
    "\n",
    "x = torch.ones([batch_size, in_channels, resolution, resolution]).cuda()\n",
    "\n",
    "print(summary(generator,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',generator(x).shape)\n",
    "print('===================================')\n",
    "\n",
    "generator.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Conv2d-1     [16, 128, 128, 128]           4,224           4,224\n",
      "       LeakyReLU-2     [16, 128, 128, 128]               0               0\n",
      "          Conv2d-3       [16, 256, 64, 64]         524,544         524,544\n",
      "       LeakyReLU-4       [16, 256, 64, 64]               0               0\n",
      "          Conv2d-5       [16, 512, 32, 32]       2,097,664       2,097,664\n",
      "       LeakyReLU-6       [16, 512, 32, 32]               0               0\n",
      "          Conv2d-7       [16, 512, 31, 31]       4,194,816       4,194,816\n",
      "       LeakyReLU-8       [16, 512, 31, 31]               0               0\n",
      "          Conv2d-9         [16, 1, 30, 30]           8,193           8,193\n",
      "===========================================================================\n",
      "Total params: 6,829,441\n",
      "Trainable params: 6,829,441\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "input: torch.Size([16, 2, 256, 256])\n",
      "output: torch.Size([16, 1, 30, 30])\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelDiscriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (out_conv): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = PixelDiscriminator(2).to(device)\n",
    "\n",
    "x = torch.ones([batch_size, 2, resolution, resolution]).cuda()\n",
    "\n",
    "print(summary(discriminator,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',discriminator(x).shape)\n",
    "print('===================================')\n",
    "\n",
    "discriminator.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = 0.0002\n",
    "d_lr = 0.00002\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "# G loss\n",
    "coefs = [1, 1, 1, 1, 0.05]\n",
    "mse_loss_1 = Intensity_Loss().to(device)\n",
    "mse_loss_2 = Intensity_Loss().to(device)\n",
    "gd_loss_1 = Gradient_Loss(1).to(device)\n",
    "gd_loss_2 = Gradient_Loss(1).to(device)\n",
    "adv_loss = Adversarial_Loss().to(device)\n",
    "\n",
    "# D loss\n",
    "disc_loss = Discriminate_Loss().to(device)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=d_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './weight/gan_generator'\n",
    "BEST_SAVE_PATH = './weight/gan_generator_best.pth'\n",
    "LAST_SAVE_PATH = './weight/gan_generator_last.pth'\n",
    "train_g_losses = []\n",
    "valid_g_losses = []\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getG_loss(out, sub, out_target, sub_target, fake, coefs): \n",
    "    '''\n",
    "    coefs = [1, 1, 1, 1, 0.05]\n",
    "    '''\n",
    "\n",
    "    mse_out = mse_loss_1(out, out_target)\n",
    "    mse_sub = mse_loss_2(sub, sub_target)\n",
    "    gdl_out = gd_loss_1(out, out_target)\n",
    "    gdl_sub = gd_loss_2(sub, sub_target)\n",
    "    adl = adv_loss(fake)\n",
    "\n",
    "    loss_gen = coefs[0] * mse_out + \\\n",
    "               coefs[1] * mse_sub + \\\n",
    "               coefs[2] * gdl_out + \\\n",
    "               coefs[3] * gdl_sub + \\\n",
    "               coefs[4] * adl\n",
    "    \n",
    "    return loss_gen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getD_loss(real, fake):\n",
    "    return disc_loss(real, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] G Loss: 0.470755934715271 / D Loss: 0.2985592484474182\n",
      "[100] G Loss: 0.2845551669597626 / D Loss: 0.24432380497455597\n",
      "-------------------------------------------------------------------\n",
      "[1/200] Train G Loss: 0.67126\t\n",
      "[1/200] Valiation G Loss: 0.28277\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.19271142780780792 / D Loss: 0.24897313117980957\n",
      "[100] G Loss: 0.1747616082429886 / D Loss: 0.24887657165527344\n",
      "-------------------------------------------------------------------\n",
      "[2/200] Train G Loss: 0.19227\t\n",
      "[2/200] Valiation G Loss: 0.20882\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.17894597351551056 / D Loss: 0.23118042945861816\n",
      "[100] G Loss: 0.1527455747127533 / D Loss: 0.24683676660060883\n",
      "-------------------------------------------------------------------\n",
      "[3/200] Train G Loss: 0.15847\t\n",
      "[3/200] Valiation G Loss: 0.18662\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13851046562194824 / D Loss: 0.24405476450920105\n",
      "[100] G Loss: 0.14363904297351837 / D Loss: 0.24104034900665283\n",
      "-------------------------------------------------------------------\n",
      "[4/200] Train G Loss: 0.14304\t\n",
      "[4/200] Valiation G Loss: 0.17286\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13724826276302338 / D Loss: 0.2385997474193573\n",
      "[100] G Loss: 0.13912120461463928 / D Loss: 0.23495909571647644\n",
      "-------------------------------------------------------------------\n",
      "[5/200] Train G Loss: 0.13670\t\n",
      "[5/200] Valiation G Loss: 0.16859\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12568598985671997 / D Loss: 0.25774911046028137\n",
      "[100] G Loss: 0.13064269721508026 / D Loss: 0.22497674822807312\n",
      "-------------------------------------------------------------------\n",
      "[6/200] Train G Loss: 0.13319\t\n",
      "[6/200] Valiation G Loss: 0.16345\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.14339160919189453 / D Loss: 0.2026517391204834\n",
      "[100] G Loss: 0.12484332174062729 / D Loss: 0.22692462801933289\n",
      "-------------------------------------------------------------------\n",
      "[7/200] Train G Loss: 0.13091\t\n",
      "[7/200] Valiation G Loss: 0.16160\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13223455846309662 / D Loss: 0.1992403268814087\n",
      "[100] G Loss: 0.1267860382795334 / D Loss: 0.21597561240196228\n",
      "-------------------------------------------------------------------\n",
      "[8/200] Train G Loss: 0.13005\t\n",
      "[8/200] Valiation G Loss: 0.16233\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13087999820709229 / D Loss: 0.20531313121318817\n",
      "[100] G Loss: 0.12623675167560577 / D Loss: 0.196446031332016\n",
      "-------------------------------------------------------------------\n",
      "[9/200] Train G Loss: 0.12947\t\n",
      "[9/200] Valiation G Loss: 0.16112\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12409092485904694 / D Loss: 0.19931182265281677\n",
      "[100] G Loss: 0.13314753770828247 / D Loss: 0.20113582909107208\n",
      "-------------------------------------------------------------------\n",
      "[10/200] Train G Loss: 0.12889\t\n",
      "<< Best model save at [10] epoch! >>\n",
      "<< model save at [10] epoch! >>\n",
      "[10/200] Valiation G Loss: 0.15712\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12268153578042984 / D Loss: 0.20489458739757538\n",
      "[100] G Loss: 0.13184252381324768 / D Loss: 0.21141359210014343\n",
      "-------------------------------------------------------------------\n",
      "[11/200] Train G Loss: 0.12803\t\n",
      "[11/200] Valiation G Loss: 0.15767\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1235329806804657 / D Loss: 0.21041852235794067\n",
      "[100] G Loss: 0.12388288974761963 / D Loss: 0.2457219511270523\n",
      "-------------------------------------------------------------------\n",
      "[12/200] Train G Loss: 0.12944\t\n",
      "[12/200] Valiation G Loss: 0.15731\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12631340324878693 / D Loss: 0.223390132188797\n",
      "[100] G Loss: 0.13163164258003235 / D Loss: 0.19148918986320496\n",
      "-------------------------------------------------------------------\n",
      "[13/200] Train G Loss: 0.12750\t\n",
      "[13/200] Valiation G Loss: 0.16003\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12791700661182404 / D Loss: 0.2628437578678131\n",
      "[100] G Loss: 0.12092463672161102 / D Loss: 0.22051836550235748\n",
      "-------------------------------------------------------------------\n",
      "[14/200] Train G Loss: 0.12691\t\n",
      "<< Best model save at [14] epoch! >>\n",
      "[14/200] Valiation G Loss: 0.15647\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12774798274040222 / D Loss: 0.19442501664161682\n",
      "[100] G Loss: 0.12242994457483292 / D Loss: 0.22445568442344666\n",
      "-------------------------------------------------------------------\n",
      "[15/200] Train G Loss: 0.12555\t\n",
      "<< model save at [15] epoch! >>\n",
      "[15/200] Valiation G Loss: 0.15708\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.13267289102077484 / D Loss: 0.25834089517593384\n",
      "[100] G Loss: 0.11391132324934006 / D Loss: 0.3330932855606079\n",
      "-------------------------------------------------------------------\n",
      "[16/200] Train G Loss: 0.12496\t\n",
      "<< Best model save at [16] epoch! >>\n",
      "[16/200] Valiation G Loss: 0.15460\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.127386212348938 / D Loss: 0.20461595058441162\n",
      "[100] G Loss: 0.13320311903953552 / D Loss: 0.24509301781654358\n",
      "-------------------------------------------------------------------\n",
      "[17/200] Train G Loss: 0.12437\t\n",
      "<< Best model save at [17] epoch! >>\n",
      "[17/200] Valiation G Loss: 0.15314\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.126401886343956 / D Loss: 0.23476874828338623\n",
      "[100] G Loss: 0.12692102789878845 / D Loss: 0.2345696985721588\n",
      "-------------------------------------------------------------------\n",
      "[18/200] Train G Loss: 0.12383\t\n",
      "[18/200] Valiation G Loss: 0.15446\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12516699731349945 / D Loss: 0.2050628960132599\n",
      "[100] G Loss: 0.13283342123031616 / D Loss: 0.22849854826927185\n",
      "-------------------------------------------------------------------\n",
      "[19/200] Train G Loss: 0.12330\t\n",
      "<< Best model save at [19] epoch! >>\n",
      "[19/200] Valiation G Loss: 0.15243\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12255492806434631 / D Loss: 0.23646704852581024\n",
      "[100] G Loss: 0.12090417742729187 / D Loss: 0.18954594433307648\n",
      "-------------------------------------------------------------------\n",
      "[20/200] Train G Loss: 0.12306\t\n",
      "<< model save at [20] epoch! >>\n",
      "[20/200] Valiation G Loss: 0.15329\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12609705328941345 / D Loss: 0.2704717516899109\n",
      "[100] G Loss: 0.1217542365193367 / D Loss: 0.246755450963974\n",
      "-------------------------------------------------------------------\n",
      "[21/200] Train G Loss: 0.12239\t\n",
      "[21/200] Valiation G Loss: 0.15294\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12130013108253479 / D Loss: 0.20836123824119568\n",
      "[100] G Loss: 0.12119252979755402 / D Loss: 0.247615247964859\n",
      "-------------------------------------------------------------------\n",
      "[22/200] Train G Loss: 0.12203\t\n",
      "[22/200] Valiation G Loss: 0.15641\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12467299401760101 / D Loss: 0.238690584897995\n",
      "[100] G Loss: 0.1172177642583847 / D Loss: 0.24319973587989807\n",
      "-------------------------------------------------------------------\n",
      "[23/200] Train G Loss: 0.12180\t\n",
      "<< Best model save at [23] epoch! >>\n",
      "[23/200] Valiation G Loss: 0.15215\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11732606589794159 / D Loss: 0.22712621092796326\n",
      "[100] G Loss: 0.12202052772045135 / D Loss: 0.19926181435585022\n",
      "-------------------------------------------------------------------\n",
      "[24/200] Train G Loss: 0.12127\t\n",
      "[24/200] Valiation G Loss: 0.15274\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12521855533123016 / D Loss: 0.24342545866966248\n",
      "[100] G Loss: 0.1269669085741043 / D Loss: 0.23014137148857117\n",
      "-------------------------------------------------------------------\n",
      "[25/200] Train G Loss: 0.12118\t\n",
      "<< Best model save at [25] epoch! >>\n",
      "<< model save at [25] epoch! >>\n",
      "[25/200] Valiation G Loss: 0.15156\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11835306882858276 / D Loss: 0.23034065961837769\n",
      "[100] G Loss: 0.11612568795681 / D Loss: 0.22144007682800293\n",
      "-------------------------------------------------------------------\n",
      "[26/200] Train G Loss: 0.12076\t\n",
      "[26/200] Valiation G Loss: 0.15171\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11249261349439621 / D Loss: 0.25411033630371094\n",
      "[100] G Loss: 0.11148805916309357 / D Loss: 0.21900492906570435\n",
      "-------------------------------------------------------------------\n",
      "[27/200] Train G Loss: 0.12042\t\n",
      "<< Best model save at [27] epoch! >>\n",
      "[27/200] Valiation G Loss: 0.15103\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12878844141960144 / D Loss: 0.22747302055358887\n",
      "[100] G Loss: 0.12847816944122314 / D Loss: 0.23519358038902283\n",
      "-------------------------------------------------------------------\n",
      "[28/200] Train G Loss: 0.12020\t\n",
      "[28/200] Valiation G Loss: 0.15306\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12190628051757812 / D Loss: 0.23864179849624634\n",
      "[100] G Loss: 0.12507635354995728 / D Loss: 0.21664109826087952\n",
      "-------------------------------------------------------------------\n",
      "[29/200] Train G Loss: 0.11998\t\n",
      "<< Best model save at [29] epoch! >>\n",
      "[29/200] Valiation G Loss: 0.15079\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11829425394535065 / D Loss: 0.24250918626785278\n",
      "[100] G Loss: 0.1204216256737709 / D Loss: 0.2512177526950836\n",
      "-------------------------------------------------------------------\n",
      "[30/200] Train G Loss: 0.11967\t\n",
      "<< model save at [30] epoch! >>\n",
      "[30/200] Valiation G Loss: 0.15234\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12236210703849792 / D Loss: 0.21654075384140015\n",
      "[100] G Loss: 0.11834647506475449 / D Loss: 0.22376276552677155\n",
      "-------------------------------------------------------------------\n",
      "[31/200] Train G Loss: 0.11957\t\n",
      "[31/200] Valiation G Loss: 0.15495\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1279977411031723 / D Loss: 0.22165483236312866\n",
      "[100] G Loss: 0.1189471036195755 / D Loss: 0.2274206429719925\n",
      "-------------------------------------------------------------------\n",
      "[32/200] Train G Loss: 0.11903\t\n",
      "<< Best model save at [32] epoch! >>\n",
      "[32/200] Valiation G Loss: 0.14764\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1194787472486496 / D Loss: 0.23994192481040955\n",
      "[100] G Loss: 0.11894432455301285 / D Loss: 0.23098412156105042\n",
      "-------------------------------------------------------------------\n",
      "[33/200] Train G Loss: 0.11883\t\n",
      "[33/200] Valiation G Loss: 0.15102\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12335377186536789 / D Loss: 0.23362578451633453\n",
      "[100] G Loss: 0.11488988250494003 / D Loss: 0.23411256074905396\n",
      "-------------------------------------------------------------------\n",
      "[34/200] Train G Loss: 0.11852\t\n",
      "[34/200] Valiation G Loss: 0.14839\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12672646343708038 / D Loss: 0.24046535789966583\n",
      "[100] G Loss: 0.1271923929452896 / D Loss: 0.21606075763702393\n",
      "-------------------------------------------------------------------\n",
      "[35/200] Train G Loss: 0.11822\t\n",
      "<< model save at [35] epoch! >>\n",
      "[35/200] Valiation G Loss: 0.14960\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11965391039848328 / D Loss: 0.23920166492462158\n",
      "[100] G Loss: 0.11804453283548355 / D Loss: 0.24039079248905182\n",
      "-------------------------------------------------------------------\n",
      "[36/200] Train G Loss: 0.11835\t\n",
      "[36/200] Valiation G Loss: 0.14769\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11393307149410248 / D Loss: 0.24814319610595703\n",
      "[100] G Loss: 0.12307652831077576 / D Loss: 0.23437348008155823\n",
      "-------------------------------------------------------------------\n",
      "[37/200] Train G Loss: 0.11891\t\n",
      "[37/200] Valiation G Loss: 0.14993\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11358471214771271 / D Loss: 0.23581740260124207\n",
      "[100] G Loss: 0.11493850499391556 / D Loss: 0.22585782408714294\n",
      "-------------------------------------------------------------------\n",
      "[38/200] Train G Loss: 0.11791\t\n",
      "[38/200] Valiation G Loss: 0.15087\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12238939106464386 / D Loss: 0.23195621371269226\n",
      "[100] G Loss: 0.11647169291973114 / D Loss: 0.2202993929386139\n",
      "-------------------------------------------------------------------\n",
      "[39/200] Train G Loss: 0.11823\t\n",
      "[39/200] Valiation G Loss: 0.15035\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1169203594326973 / D Loss: 0.2221006453037262\n",
      "[100] G Loss: 0.12188587337732315 / D Loss: 0.24461674690246582\n",
      "-------------------------------------------------------------------\n",
      "[40/200] Train G Loss: 0.11743\t\n",
      "<< Best model save at [40] epoch! >>\n",
      "<< model save at [40] epoch! >>\n",
      "[40/200] Valiation G Loss: 0.14656\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11204565316438675 / D Loss: 0.2333579957485199\n",
      "[100] G Loss: 0.11421938240528107 / D Loss: 0.22023966908454895\n",
      "-------------------------------------------------------------------\n",
      "[41/200] Train G Loss: 0.11732\t\n",
      "[41/200] Valiation G Loss: 0.14991\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1163841262459755 / D Loss: 0.24727989733219147\n",
      "[100] G Loss: 0.12191393971443176 / D Loss: 0.2336086928844452\n",
      "-------------------------------------------------------------------\n",
      "[42/200] Train G Loss: 0.11710\t\n",
      "[42/200] Valiation G Loss: 0.14834\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12063366174697876 / D Loss: 0.22493553161621094\n",
      "[100] G Loss: 0.11374719440937042 / D Loss: 0.22965767979621887\n",
      "-------------------------------------------------------------------\n",
      "[43/200] Train G Loss: 0.11680\t\n",
      "[43/200] Valiation G Loss: 0.14666\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12408949434757233 / D Loss: 0.2297419011592865\n",
      "[100] G Loss: 0.11975421756505966 / D Loss: 0.2356201559305191\n",
      "-------------------------------------------------------------------\n",
      "[44/200] Train G Loss: 0.11662\t\n",
      "<< Best model save at [44] epoch! >>\n",
      "[44/200] Valiation G Loss: 0.14603\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.112356998026371 / D Loss: 0.23055003583431244\n",
      "[100] G Loss: 0.1234021931886673 / D Loss: 0.23358154296875\n",
      "-------------------------------------------------------------------\n",
      "[45/200] Train G Loss: 0.11657\t\n",
      "<< model save at [45] epoch! >>\n",
      "[45/200] Valiation G Loss: 0.14998\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11641022562980652 / D Loss: 0.22545522451400757\n",
      "[100] G Loss: 0.11472059041261673 / D Loss: 0.21507850289344788\n",
      "-------------------------------------------------------------------\n",
      "[46/200] Train G Loss: 0.11624\t\n",
      "[46/200] Valiation G Loss: 0.14681\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12165994197130203 / D Loss: 0.22436639666557312\n",
      "[100] G Loss: 0.11567403376102448 / D Loss: 0.24716712534427643\n",
      "-------------------------------------------------------------------\n",
      "[47/200] Train G Loss: 0.11625\t\n",
      "[47/200] Valiation G Loss: 0.14783\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12319842725992203 / D Loss: 0.23688088357448578\n",
      "[100] G Loss: 0.1170058473944664 / D Loss: 0.22409915924072266\n",
      "-------------------------------------------------------------------\n",
      "[48/200] Train G Loss: 0.11609\t\n",
      "[48/200] Valiation G Loss: 0.14844\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12010011076927185 / D Loss: 0.2092641443014145\n",
      "[100] G Loss: 0.11040529608726501 / D Loss: 0.2368621975183487\n",
      "-------------------------------------------------------------------\n",
      "[49/200] Train G Loss: 0.11577\t\n",
      "<< Best model save at [49] epoch! >>\n",
      "[49/200] Valiation G Loss: 0.14541\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11241963505744934 / D Loss: 0.25271323323249817\n",
      "[100] G Loss: 0.12113276869058609 / D Loss: 0.22304221987724304\n",
      "-------------------------------------------------------------------\n",
      "[50/200] Train G Loss: 0.11570\t\n",
      "<< model save at [50] epoch! >>\n",
      "[50/200] Valiation G Loss: 0.14583\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10775528103113174 / D Loss: 0.23546771705150604\n",
      "[100] G Loss: 0.12090574949979782 / D Loss: 0.22434735298156738\n",
      "-------------------------------------------------------------------\n",
      "[51/200] Train G Loss: 0.11566\t\n",
      "<< Best model save at [51] epoch! >>\n",
      "[51/200] Valiation G Loss: 0.14538\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1132010966539383 / D Loss: 0.23992851376533508\n",
      "[100] G Loss: 0.11275766044855118 / D Loss: 0.24428558349609375\n",
      "-------------------------------------------------------------------\n",
      "[52/200] Train G Loss: 0.11540\t\n",
      "[52/200] Valiation G Loss: 0.14929\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11159955710172653 / D Loss: 0.24359554052352905\n",
      "[100] G Loss: 0.10841470211744308 / D Loss: 0.2372414767742157\n",
      "-------------------------------------------------------------------\n",
      "[53/200] Train G Loss: 0.11559\t\n",
      "<< Best model save at [53] epoch! >>\n",
      "[53/200] Valiation G Loss: 0.14377\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1222393587231636 / D Loss: 0.2107660174369812\n",
      "[100] G Loss: 0.1111510843038559 / D Loss: 0.233970046043396\n",
      "-------------------------------------------------------------------\n",
      "[54/200] Train G Loss: 0.11513\t\n",
      "[54/200] Valiation G Loss: 0.14710\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12296690791845322 / D Loss: 0.20600154995918274\n",
      "[100] G Loss: 0.1147199347615242 / D Loss: 0.2391812652349472\n",
      "-------------------------------------------------------------------\n",
      "[55/200] Train G Loss: 0.11517\t\n",
      "<< model save at [55] epoch! >>\n",
      "[55/200] Valiation G Loss: 0.14433\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11454518139362335 / D Loss: 0.2488863468170166\n",
      "[100] G Loss: 0.11642738431692123 / D Loss: 0.23222777247428894\n",
      "-------------------------------------------------------------------\n",
      "[56/200] Train G Loss: 0.11486\t\n",
      "[56/200] Valiation G Loss: 0.14447\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.111583411693573 / D Loss: 0.23493775725364685\n",
      "[100] G Loss: 0.12540769577026367 / D Loss: 0.22408217191696167\n",
      "-------------------------------------------------------------------\n",
      "[57/200] Train G Loss: 0.11488\t\n",
      "[57/200] Valiation G Loss: 0.14643\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11536422371864319 / D Loss: 0.22960424423217773\n",
      "[100] G Loss: 0.11385224759578705 / D Loss: 0.22784510254859924\n",
      "-------------------------------------------------------------------\n",
      "[58/200] Train G Loss: 0.11466\t\n",
      "[58/200] Valiation G Loss: 0.14684\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1114305704832077 / D Loss: 0.23809130489826202\n",
      "[100] G Loss: 0.11918845772743225 / D Loss: 0.2316153645515442\n",
      "-------------------------------------------------------------------\n",
      "[59/200] Train G Loss: 0.11456\t\n",
      "[59/200] Valiation G Loss: 0.14701\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11806608736515045 / D Loss: 0.22612503170967102\n",
      "[100] G Loss: 0.1164098009467125 / D Loss: 0.22695866227149963\n",
      "-------------------------------------------------------------------\n",
      "[60/200] Train G Loss: 0.11446\t\n",
      "<< model save at [60] epoch! >>\n",
      "[60/200] Valiation G Loss: 0.14515\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11010577529668808 / D Loss: 0.23545178771018982\n",
      "[100] G Loss: 0.113409124314785 / D Loss: 0.2516384720802307\n",
      "-------------------------------------------------------------------\n",
      "[61/200] Train G Loss: 0.11439\t\n",
      "[61/200] Valiation G Loss: 0.14568\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12487602978944778 / D Loss: 0.22402769327163696\n",
      "[100] G Loss: 0.11024512350559235 / D Loss: 0.25278303027153015\n",
      "-------------------------------------------------------------------\n",
      "[62/200] Train G Loss: 0.11534\t\n",
      "[62/200] Valiation G Loss: 0.14922\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11238618940114975 / D Loss: 0.23224151134490967\n",
      "[100] G Loss: 0.11161606758832932 / D Loss: 0.24446839094161987\n",
      "-------------------------------------------------------------------\n",
      "[63/200] Train G Loss: 0.11491\t\n",
      "<< Best model save at [63] epoch! >>\n",
      "[63/200] Valiation G Loss: 0.14267\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1029592752456665 / D Loss: 0.23453289270401\n",
      "[100] G Loss: 0.11534376442432404 / D Loss: 0.22394683957099915\n",
      "-------------------------------------------------------------------\n",
      "[64/200] Train G Loss: 0.11438\t\n",
      "[64/200] Valiation G Loss: 0.14532\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12242724746465683 / D Loss: 0.21069931983947754\n",
      "[100] G Loss: 0.11650773882865906 / D Loss: 0.22585010528564453\n",
      "-------------------------------------------------------------------\n",
      "[65/200] Train G Loss: 0.11414\t\n",
      "<< model save at [65] epoch! >>\n",
      "[65/200] Valiation G Loss: 0.14596\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11103253066539764 / D Loss: 0.24541205167770386\n",
      "[100] G Loss: 0.10781995207071304 / D Loss: 0.2387724220752716\n",
      "-------------------------------------------------------------------\n",
      "[66/200] Train G Loss: 0.11416\t\n",
      "[66/200] Valiation G Loss: 0.14421\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1149073913693428 / D Loss: 0.21950744092464447\n",
      "[100] G Loss: 0.11140447109937668 / D Loss: 0.2437151074409485\n",
      "-------------------------------------------------------------------\n",
      "[67/200] Train G Loss: 0.11409\t\n",
      "[67/200] Valiation G Loss: 0.14869\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1257852017879486 / D Loss: 0.19691991806030273\n",
      "[100] G Loss: 0.11650381982326508 / D Loss: 0.23129387199878693\n",
      "-------------------------------------------------------------------\n",
      "[68/200] Train G Loss: 0.11558\t\n",
      "[68/200] Valiation G Loss: 0.14357\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12074900418519974 / D Loss: 0.22382333874702454\n",
      "[100] G Loss: 0.11528225988149643 / D Loss: 0.245608389377594\n",
      "-------------------------------------------------------------------\n",
      "[69/200] Train G Loss: 0.11454\t\n",
      "[69/200] Valiation G Loss: 0.14467\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11619353294372559 / D Loss: 0.22724682092666626\n",
      "[100] G Loss: 0.11453615874052048 / D Loss: 0.2290405035018921\n",
      "-------------------------------------------------------------------\n",
      "[70/200] Train G Loss: 0.11421\t\n",
      "<< model save at [70] epoch! >>\n",
      "[70/200] Valiation G Loss: 0.14683\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11502557247877121 / D Loss: 0.22607707977294922\n",
      "[100] G Loss: 0.11455778777599335 / D Loss: 0.25265806913375854\n",
      "-------------------------------------------------------------------\n",
      "[71/200] Train G Loss: 0.11420\t\n",
      "[71/200] Valiation G Loss: 0.14319\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11198532581329346 / D Loss: 0.23356205224990845\n",
      "[100] G Loss: 0.11560562252998352 / D Loss: 0.22800694406032562\n",
      "-------------------------------------------------------------------\n",
      "[72/200] Train G Loss: 0.11398\t\n",
      "[72/200] Valiation G Loss: 0.14488\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1164458841085434 / D Loss: 0.22281020879745483\n",
      "[100] G Loss: 0.11857035756111145 / D Loss: 0.234859436750412\n",
      "-------------------------------------------------------------------\n",
      "[73/200] Train G Loss: 0.11390\t\n",
      "[73/200] Valiation G Loss: 0.14406\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1094929426908493 / D Loss: 0.23722520470619202\n",
      "[100] G Loss: 0.11573604494333267 / D Loss: 0.23841804265975952\n",
      "-------------------------------------------------------------------\n",
      "[74/200] Train G Loss: 0.11386\t\n",
      "[74/200] Valiation G Loss: 0.14419\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10615354776382446 / D Loss: 0.22767701745033264\n",
      "[100] G Loss: 0.11511404812335968 / D Loss: 0.23522719740867615\n",
      "-------------------------------------------------------------------\n",
      "[75/200] Train G Loss: 0.11462\t\n",
      "<< model save at [75] epoch! >>\n",
      "[75/200] Valiation G Loss: 0.14395\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11749918758869171 / D Loss: 0.2296852469444275\n",
      "[100] G Loss: 0.1144903302192688 / D Loss: 0.2271464467048645\n",
      "-------------------------------------------------------------------\n",
      "[76/200] Train G Loss: 0.11395\t\n",
      "[76/200] Valiation G Loss: 0.14332\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10815256088972092 / D Loss: 0.2534473240375519\n",
      "[100] G Loss: 0.11238499730825424 / D Loss: 0.23217201232910156\n",
      "-------------------------------------------------------------------\n",
      "[77/200] Train G Loss: 0.11383\t\n",
      "[77/200] Valiation G Loss: 0.14553\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10836414992809296 / D Loss: 0.23695515096187592\n",
      "[100] G Loss: 0.1163865253329277 / D Loss: 0.22045865654945374\n",
      "-------------------------------------------------------------------\n",
      "[78/200] Train G Loss: 0.11372\t\n",
      "[78/200] Valiation G Loss: 0.14475\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1122451201081276 / D Loss: 0.23309215903282166\n",
      "[100] G Loss: 0.1194886565208435 / D Loss: 0.22019246220588684\n",
      "-------------------------------------------------------------------\n",
      "[79/200] Train G Loss: 0.11366\t\n",
      "[79/200] Valiation G Loss: 0.14526\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10974884033203125 / D Loss: 0.2414516657590866\n",
      "[100] G Loss: 0.1190510243177414 / D Loss: 0.2419613003730774\n",
      "-------------------------------------------------------------------\n",
      "[80/200] Train G Loss: 0.11365\t\n",
      "<< model save at [80] epoch! >>\n",
      "[80/200] Valiation G Loss: 0.14431\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10880614817142487 / D Loss: 0.23313114047050476\n",
      "[100] G Loss: 0.11132822930812836 / D Loss: 0.23899909853935242\n",
      "-------------------------------------------------------------------\n",
      "[81/200] Train G Loss: 0.11371\t\n",
      "[81/200] Valiation G Loss: 0.14415\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11646033078432083 / D Loss: 0.21658828854560852\n",
      "[100] G Loss: 0.11411888152360916 / D Loss: 0.22138264775276184\n",
      "-------------------------------------------------------------------\n",
      "[82/200] Train G Loss: 0.11360\t\n",
      "[82/200] Valiation G Loss: 0.14471\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10980202257633209 / D Loss: 0.26291173696517944\n",
      "[100] G Loss: 0.11597755551338196 / D Loss: 0.22691047191619873\n",
      "-------------------------------------------------------------------\n",
      "[83/200] Train G Loss: 0.11360\t\n",
      "[83/200] Valiation G Loss: 0.14375\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11604833602905273 / D Loss: 0.22623446583747864\n",
      "[100] G Loss: 0.119404636323452 / D Loss: 0.21737687289714813\n",
      "-------------------------------------------------------------------\n",
      "[84/200] Train G Loss: 0.11370\t\n",
      "[84/200] Valiation G Loss: 0.14489\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11546628177165985 / D Loss: 0.21269817650318146\n",
      "[100] G Loss: 0.11175475269556046 / D Loss: 0.23077502846717834\n",
      "-------------------------------------------------------------------\n",
      "[85/200] Train G Loss: 0.11354\t\n",
      "<< model save at [85] epoch! >>\n",
      "[85/200] Valiation G Loss: 0.14569\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11165978759527206 / D Loss: 0.22325852513313293\n",
      "[100] G Loss: 0.10911952704191208 / D Loss: 0.23004382848739624\n",
      "-------------------------------------------------------------------\n",
      "[86/200] Train G Loss: 0.11360\t\n",
      "[86/200] Valiation G Loss: 0.14767\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11756743490695953 / D Loss: 0.2271629422903061\n",
      "[100] G Loss: 0.1079949215054512 / D Loss: 0.24160145223140717\n",
      "-------------------------------------------------------------------\n",
      "[87/200] Train G Loss: 0.11346\t\n",
      "[87/200] Valiation G Loss: 0.14294\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10707059502601624 / D Loss: 0.24167832732200623\n",
      "[100] G Loss: 0.11476379632949829 / D Loss: 0.2322233021259308\n",
      "-------------------------------------------------------------------\n",
      "[88/200] Train G Loss: 0.11343\t\n",
      "[88/200] Valiation G Loss: 0.14309\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11301101744174957 / D Loss: 0.2310841679573059\n",
      "[100] G Loss: 0.11080531030893326 / D Loss: 0.23826664686203003\n",
      "-------------------------------------------------------------------\n",
      "[89/200] Train G Loss: 0.11407\t\n",
      "[89/200] Valiation G Loss: 0.15643\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10856943577528 / D Loss: 0.2379525750875473\n",
      "[100] G Loss: 0.11346393078565598 / D Loss: 0.233158677816391\n",
      "-------------------------------------------------------------------\n",
      "[90/200] Train G Loss: 0.11579\t\n",
      "<< model save at [90] epoch! >>\n",
      "[90/200] Valiation G Loss: 0.14734\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11430978775024414 / D Loss: 0.25781840085983276\n",
      "[100] G Loss: 0.1152835339307785 / D Loss: 0.2220114916563034\n",
      "-------------------------------------------------------------------\n",
      "[91/200] Train G Loss: 0.11398\t\n",
      "[91/200] Valiation G Loss: 0.14637\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1144121065735817 / D Loss: 0.23434334993362427\n",
      "[100] G Loss: 0.11432555317878723 / D Loss: 0.2326175421476364\n",
      "-------------------------------------------------------------------\n",
      "[92/200] Train G Loss: 0.11388\t\n",
      "[92/200] Valiation G Loss: 0.14494\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10818450152873993 / D Loss: 0.22504189610481262\n",
      "[100] G Loss: 0.12348786741495132 / D Loss: 0.20187707245349884\n",
      "-------------------------------------------------------------------\n",
      "[93/200] Train G Loss: 0.11384\t\n",
      "[93/200] Valiation G Loss: 0.14401\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11807417869567871 / D Loss: 0.21923166513442993\n",
      "[100] G Loss: 0.1088031679391861 / D Loss: 0.2387823462486267\n",
      "-------------------------------------------------------------------\n",
      "[94/200] Train G Loss: 0.11366\t\n",
      "[94/200] Valiation G Loss: 0.14686\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10941188782453537 / D Loss: 0.23979787528514862\n",
      "[100] G Loss: 0.11012710630893707 / D Loss: 0.23453393578529358\n",
      "-------------------------------------------------------------------\n",
      "[95/200] Train G Loss: 0.11360\t\n",
      "<< model save at [95] epoch! >>\n",
      "[95/200] Valiation G Loss: 0.14271\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11144442856311798 / D Loss: 0.22843612730503082\n",
      "[100] G Loss: 0.1156977191567421 / D Loss: 0.22313013672828674\n",
      "-------------------------------------------------------------------\n",
      "[96/200] Train G Loss: 0.11371\t\n",
      "[96/200] Valiation G Loss: 0.14475\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11222933232784271 / D Loss: 0.24867257475852966\n",
      "[100] G Loss: 0.10756991803646088 / D Loss: 0.2663134038448334\n",
      "-------------------------------------------------------------------\n",
      "[97/200] Train G Loss: 0.11359\t\n",
      "[97/200] Valiation G Loss: 0.14395\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1173490434885025 / D Loss: 0.21111465990543365\n",
      "[100] G Loss: 0.11915518343448639 / D Loss: 0.21274614334106445\n",
      "-------------------------------------------------------------------\n",
      "[98/200] Train G Loss: 0.11355\t\n",
      "[98/200] Valiation G Loss: 0.14452\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11152982711791992 / D Loss: 0.22688710689544678\n",
      "[100] G Loss: 0.12069156020879745 / D Loss: 0.21866300702095032\n",
      "-------------------------------------------------------------------\n",
      "[99/200] Train G Loss: 0.11342\t\n",
      "[99/200] Valiation G Loss: 0.14478\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10852476954460144 / D Loss: 0.23279470205307007\n",
      "[100] G Loss: 0.11289966106414795 / D Loss: 0.22996094822883606\n",
      "-------------------------------------------------------------------\n",
      "[100/200] Train G Loss: 0.11369\t\n",
      "<< model save at [100] epoch! >>\n",
      "[100/200] Valiation G Loss: 0.14376\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11736379563808441 / D Loss: 0.22420817613601685\n",
      "[100] G Loss: 0.11841706186532974 / D Loss: 0.2163107991218567\n",
      "-------------------------------------------------------------------\n",
      "[101/200] Train G Loss: 0.11360\t\n",
      "[101/200] Valiation G Loss: 0.14439\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11056775599718094 / D Loss: 0.23922371864318848\n",
      "[100] G Loss: 0.11298545449972153 / D Loss: 0.25012093782424927\n",
      "-------------------------------------------------------------------\n",
      "[102/200] Train G Loss: 0.11345\t\n",
      "[102/200] Valiation G Loss: 0.14437\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12366185337305069 / D Loss: 0.21839416027069092\n",
      "[100] G Loss: 0.10570274293422699 / D Loss: 0.24614417552947998\n",
      "-------------------------------------------------------------------\n",
      "[103/200] Train G Loss: 0.11340\t\n",
      "<< Best model save at [103] epoch! >>\n",
      "[103/200] Valiation G Loss: 0.14261\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11033150553703308 / D Loss: 0.23255346715450287\n",
      "[100] G Loss: 0.11969556659460068 / D Loss: 0.22014275193214417\n",
      "-------------------------------------------------------------------\n",
      "[104/200] Train G Loss: 0.11340\t\n",
      "[104/200] Valiation G Loss: 0.14594\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1122882068157196 / D Loss: 0.24595259130001068\n",
      "[100] G Loss: 0.11968841403722763 / D Loss: 0.23002305626869202\n",
      "-------------------------------------------------------------------\n",
      "[105/200] Train G Loss: 0.11358\t\n",
      "<< model save at [105] epoch! >>\n",
      "[105/200] Valiation G Loss: 0.14474\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11110392212867737 / D Loss: 0.2432200014591217\n",
      "[100] G Loss: 0.11185591667890549 / D Loss: 0.2167419195175171\n",
      "-------------------------------------------------------------------\n",
      "[106/200] Train G Loss: 0.11352\t\n",
      "[106/200] Valiation G Loss: 0.14346\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11364705115556717 / D Loss: 0.23037105798721313\n",
      "[100] G Loss: 0.12257090955972672 / D Loss: 0.21354800462722778\n",
      "-------------------------------------------------------------------\n",
      "[107/200] Train G Loss: 0.11348\t\n",
      "[107/200] Valiation G Loss: 0.14311\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12005890160799026 / D Loss: 0.236922025680542\n",
      "[100] G Loss: 0.10993542522192001 / D Loss: 0.23476803302764893\n",
      "-------------------------------------------------------------------\n",
      "[108/200] Train G Loss: 0.11339\t\n",
      "[108/200] Valiation G Loss: 0.14447\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1134486049413681 / D Loss: 0.24069517850875854\n",
      "[100] G Loss: 0.10796190798282623 / D Loss: 0.22948375344276428\n",
      "-------------------------------------------------------------------\n",
      "[109/200] Train G Loss: 0.11339\t\n",
      "[109/200] Valiation G Loss: 0.14382\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11478248238563538 / D Loss: 0.22028321027755737\n",
      "[100] G Loss: 0.11117140203714371 / D Loss: 0.2351416051387787\n",
      "-------------------------------------------------------------------\n",
      "[110/200] Train G Loss: 0.11346\t\n",
      "<< Best model save at [110] epoch! >>\n",
      "<< model save at [110] epoch! >>\n",
      "[110/200] Valiation G Loss: 0.14250\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10515115410089493 / D Loss: 0.2437637448310852\n",
      "[100] G Loss: 0.10669396817684174 / D Loss: 0.23056694865226746\n",
      "-------------------------------------------------------------------\n",
      "[111/200] Train G Loss: 0.11358\t\n",
      "[111/200] Valiation G Loss: 0.14430\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11408279836177826 / D Loss: 0.2179519534111023\n",
      "[100] G Loss: 0.11539193987846375 / D Loss: 0.2038518488407135\n",
      "-------------------------------------------------------------------\n",
      "[112/200] Train G Loss: 0.11341\t\n",
      "[112/200] Valiation G Loss: 0.14361\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1095803752541542 / D Loss: 0.23869682848453522\n",
      "[100] G Loss: 0.11757092922925949 / D Loss: 0.22719764709472656\n",
      "-------------------------------------------------------------------\n",
      "[113/200] Train G Loss: 0.11343\t\n",
      "[113/200] Valiation G Loss: 0.14286\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11393779516220093 / D Loss: 0.25024640560150146\n",
      "[100] G Loss: 0.1124989464879036 / D Loss: 0.22335180640220642\n",
      "-------------------------------------------------------------------\n",
      "[114/200] Train G Loss: 0.11349\t\n",
      "[114/200] Valiation G Loss: 0.14518\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12242421507835388 / D Loss: 0.22355596721172333\n",
      "[100] G Loss: 0.10364119708538055 / D Loss: 0.25429442524909973\n",
      "-------------------------------------------------------------------\n",
      "[115/200] Train G Loss: 0.11349\t\n",
      "<< model save at [115] epoch! >>\n",
      "[115/200] Valiation G Loss: 0.14503\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1095270961523056 / D Loss: 0.2843460440635681\n",
      "[100] G Loss: 0.10895358771085739 / D Loss: 0.22788257896900177\n",
      "-------------------------------------------------------------------\n",
      "[116/200] Train G Loss: 0.11345\t\n",
      "[116/200] Valiation G Loss: 0.14313\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11164575815200806 / D Loss: 0.219320148229599\n",
      "[100] G Loss: 0.10628245770931244 / D Loss: 0.2572689950466156\n",
      "-------------------------------------------------------------------\n",
      "[117/200] Train G Loss: 0.11353\t\n",
      "[117/200] Valiation G Loss: 0.14532\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.111457958817482 / D Loss: 0.23728615045547485\n",
      "[100] G Loss: 0.10678258538246155 / D Loss: 0.28847721219062805\n",
      "-------------------------------------------------------------------\n",
      "[118/200] Train G Loss: 0.11362\t\n",
      "[118/200] Valiation G Loss: 0.14554\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11354327201843262 / D Loss: 0.24295222759246826\n",
      "[100] G Loss: 0.12087427824735641 / D Loss: 0.23595598340034485\n",
      "-------------------------------------------------------------------\n",
      "[119/200] Train G Loss: 0.11343\t\n",
      "[119/200] Valiation G Loss: 0.14511\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11377469450235367 / D Loss: 0.2631927728652954\n",
      "[100] G Loss: 0.11099245399236679 / D Loss: 0.23521405458450317\n",
      "-------------------------------------------------------------------\n",
      "[120/200] Train G Loss: 0.11348\t\n",
      "<< model save at [120] epoch! >>\n",
      "[120/200] Valiation G Loss: 0.14361\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11937281489372253 / D Loss: 0.2131679803133011\n",
      "[100] G Loss: 0.10965754836797714 / D Loss: 0.23012283444404602\n",
      "-------------------------------------------------------------------\n",
      "[121/200] Train G Loss: 0.11351\t\n",
      "[121/200] Valiation G Loss: 0.14272\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11586186289787292 / D Loss: 0.2185918092727661\n",
      "[100] G Loss: 0.10724043101072311 / D Loss: 0.2531294822692871\n",
      "-------------------------------------------------------------------\n",
      "[122/200] Train G Loss: 0.11351\t\n",
      "[122/200] Valiation G Loss: 0.14440\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11779557168483734 / D Loss: 0.23766346275806427\n",
      "[100] G Loss: 0.11857694387435913 / D Loss: 0.22972631454467773\n",
      "-------------------------------------------------------------------\n",
      "[123/200] Train G Loss: 0.11345\t\n",
      "[123/200] Valiation G Loss: 0.14425\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11993630230426788 / D Loss: 0.21809759736061096\n",
      "[100] G Loss: 0.1115846037864685 / D Loss: 0.23235496878623962\n",
      "-------------------------------------------------------------------\n",
      "[124/200] Train G Loss: 0.11359\t\n",
      "[124/200] Valiation G Loss: 0.14382\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11932676285505295 / D Loss: 0.21716439723968506\n",
      "[100] G Loss: 0.11766570061445236 / D Loss: 0.23529845476150513\n",
      "-------------------------------------------------------------------\n",
      "[125/200] Train G Loss: 0.11381\t\n",
      "<< model save at [125] epoch! >>\n",
      "[125/200] Valiation G Loss: 0.14459\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11014707386493683 / D Loss: 0.22540080547332764\n",
      "[100] G Loss: 0.10844387859106064 / D Loss: 0.2403719127178192\n",
      "-------------------------------------------------------------------\n",
      "[126/200] Train G Loss: 0.11352\t\n",
      "[126/200] Valiation G Loss: 0.14364\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11807241290807724 / D Loss: 0.23601460456848145\n",
      "[100] G Loss: 0.11990600824356079 / D Loss: 0.21230989694595337\n",
      "-------------------------------------------------------------------\n",
      "[127/200] Train G Loss: 0.11343\t\n",
      "[127/200] Valiation G Loss: 0.14620\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10923035442829132 / D Loss: 0.2176417112350464\n",
      "[100] G Loss: 0.11329048871994019 / D Loss: 0.23095226287841797\n",
      "-------------------------------------------------------------------\n",
      "[128/200] Train G Loss: 0.11352\t\n",
      "[128/200] Valiation G Loss: 0.14585\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10339786857366562 / D Loss: 0.2552310526371002\n",
      "[100] G Loss: 0.11781246215105057 / D Loss: 0.2190614640712738\n",
      "-------------------------------------------------------------------\n",
      "[129/200] Train G Loss: 0.11345\t\n",
      "[129/200] Valiation G Loss: 0.14434\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12279533594846725 / D Loss: 0.24422352015972137\n",
      "[100] G Loss: 0.11740612238645554 / D Loss: 0.2361886352300644\n",
      "-------------------------------------------------------------------\n",
      "[130/200] Train G Loss: 0.11344\t\n",
      "<< model save at [130] epoch! >>\n",
      "[130/200] Valiation G Loss: 0.14417\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11962632089853287 / D Loss: 0.21484041213989258\n",
      "[100] G Loss: 0.10863728821277618 / D Loss: 0.23499979078769684\n",
      "-------------------------------------------------------------------\n",
      "[131/200] Train G Loss: 0.11346\t\n",
      "[131/200] Valiation G Loss: 0.14405\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11502064019441605 / D Loss: 0.2469927966594696\n",
      "[100] G Loss: 0.11680054664611816 / D Loss: 0.2305397093296051\n",
      "-------------------------------------------------------------------\n",
      "[132/200] Train G Loss: 0.11354\t\n",
      "[132/200] Valiation G Loss: 0.14823\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11650697141885757 / D Loss: 0.2350410521030426\n",
      "[100] G Loss: 0.11307389289140701 / D Loss: 0.24597740173339844\n",
      "-------------------------------------------------------------------\n",
      "[133/200] Train G Loss: 0.11336\t\n",
      "<< Best model save at [133] epoch! >>\n",
      "[133/200] Valiation G Loss: 0.14189\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11330214142799377 / D Loss: 0.2430514246225357\n",
      "[100] G Loss: 0.11357995122671127 / D Loss: 0.24753829836845398\n",
      "-------------------------------------------------------------------\n",
      "[134/200] Train G Loss: 0.11337\t\n",
      "[134/200] Valiation G Loss: 0.14356\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11040311306715012 / D Loss: 0.26136618852615356\n",
      "[100] G Loss: 0.11543584614992142 / D Loss: 0.22746431827545166\n",
      "-------------------------------------------------------------------\n",
      "[135/200] Train G Loss: 0.11348\t\n",
      "<< model save at [135] epoch! >>\n",
      "[135/200] Valiation G Loss: 0.14334\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11385229229927063 / D Loss: 0.2245582938194275\n",
      "[100] G Loss: 0.11458303779363632 / D Loss: 0.23475530743598938\n",
      "-------------------------------------------------------------------\n",
      "[136/200] Train G Loss: 0.11340\t\n",
      "[136/200] Valiation G Loss: 0.14421\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10450202971696854 / D Loss: 0.25117233395576477\n",
      "[100] G Loss: 0.11592373996973038 / D Loss: 0.22708332538604736\n",
      "-------------------------------------------------------------------\n",
      "[137/200] Train G Loss: 0.11339\t\n",
      "[137/200] Valiation G Loss: 0.14298\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11295691132545471 / D Loss: 0.22401514649391174\n",
      "[100] G Loss: 0.1178901344537735 / D Loss: 0.24611181020736694\n",
      "-------------------------------------------------------------------\n",
      "[138/200] Train G Loss: 0.11350\t\n",
      "[138/200] Valiation G Loss: 0.14631\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11099068075418472 / D Loss: 0.22357967495918274\n",
      "[100] G Loss: 0.117018923163414 / D Loss: 0.2155488133430481\n",
      "-------------------------------------------------------------------\n",
      "[139/200] Train G Loss: 0.11340\t\n",
      "<< Best model save at [139] epoch! >>\n",
      "[139/200] Valiation G Loss: 0.14065\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11967872083187103 / D Loss: 0.2330246865749359\n",
      "[100] G Loss: 0.1141534075140953 / D Loss: 0.2190360277891159\n",
      "-------------------------------------------------------------------\n",
      "[140/200] Train G Loss: 0.11332\t\n",
      "<< model save at [140] epoch! >>\n",
      "[140/200] Valiation G Loss: 0.14396\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11020543426275253 / D Loss: 0.2591296434402466\n",
      "[100] G Loss: 0.11763502657413483 / D Loss: 0.21652692556381226\n",
      "-------------------------------------------------------------------\n",
      "[141/200] Train G Loss: 0.11345\t\n",
      "[141/200] Valiation G Loss: 0.14374\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10598776489496231 / D Loss: 0.2310222089290619\n",
      "[100] G Loss: 0.11604762077331543 / D Loss: 0.20676970481872559\n",
      "-------------------------------------------------------------------\n",
      "[142/200] Train G Loss: 0.11331\t\n",
      "[142/200] Valiation G Loss: 0.14427\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11573164165019989 / D Loss: 0.224215567111969\n",
      "[100] G Loss: 0.12043653428554535 / D Loss: 0.22411945462226868\n",
      "-------------------------------------------------------------------\n",
      "[143/200] Train G Loss: 0.11367\t\n",
      "[143/200] Valiation G Loss: 0.14369\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11199823766946793 / D Loss: 0.24704954028129578\n",
      "[100] G Loss: 0.11189223825931549 / D Loss: 0.22479668259620667\n",
      "-------------------------------------------------------------------\n",
      "[144/200] Train G Loss: 0.11337\t\n",
      "[144/200] Valiation G Loss: 0.14400\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12195328623056412 / D Loss: 0.20273438096046448\n",
      "[100] G Loss: 0.11923494935035706 / D Loss: 0.22681966423988342\n",
      "-------------------------------------------------------------------\n",
      "[145/200] Train G Loss: 0.11340\t\n",
      "<< model save at [145] epoch! >>\n",
      "[145/200] Valiation G Loss: 0.14355\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11692293733358383 / D Loss: 0.2414088249206543\n",
      "[100] G Loss: 0.11395730078220367 / D Loss: 0.21899062395095825\n",
      "-------------------------------------------------------------------\n",
      "[146/200] Train G Loss: 0.11371\t\n",
      "[146/200] Valiation G Loss: 0.14360\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1155623197555542 / D Loss: 0.21064120531082153\n",
      "[100] G Loss: 0.10738811641931534 / D Loss: 0.222069650888443\n",
      "-------------------------------------------------------------------\n",
      "[147/200] Train G Loss: 0.11455\t\n",
      "[147/200] Valiation G Loss: 0.14525\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.124872587621212 / D Loss: 0.19389456510543823\n",
      "[100] G Loss: 0.10927402973175049 / D Loss: 0.2667422890663147\n",
      "-------------------------------------------------------------------\n",
      "[148/200] Train G Loss: 0.11389\t\n",
      "[148/200] Valiation G Loss: 0.14825\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11050757765769958 / D Loss: 0.23662973940372467\n",
      "[100] G Loss: 0.11965898424386978 / D Loss: 0.1783987581729889\n",
      "-------------------------------------------------------------------\n",
      "[149/200] Train G Loss: 0.11373\t\n",
      "[149/200] Valiation G Loss: 0.14526\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11700598895549774 / D Loss: 0.2244608998298645\n",
      "[100] G Loss: 0.11795759201049805 / D Loss: 0.22154822945594788\n",
      "-------------------------------------------------------------------\n",
      "[150/200] Train G Loss: 0.11379\t\n",
      "<< model save at [150] epoch! >>\n",
      "[150/200] Valiation G Loss: 0.14370\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11259523779153824 / D Loss: 0.23091405630111694\n",
      "[100] G Loss: 0.11503788083791733 / D Loss: 0.2186187207698822\n",
      "-------------------------------------------------------------------\n",
      "[151/200] Train G Loss: 0.11368\t\n",
      "[151/200] Valiation G Loss: 0.14628\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11751158535480499 / D Loss: 0.23267042636871338\n",
      "[100] G Loss: 0.1142842099070549 / D Loss: 0.2137274444103241\n",
      "-------------------------------------------------------------------\n",
      "[152/200] Train G Loss: 0.11371\t\n",
      "[152/200] Valiation G Loss: 0.14577\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11864995956420898 / D Loss: 0.2133532017469406\n",
      "[100] G Loss: 0.11278726160526276 / D Loss: 0.24822622537612915\n",
      "-------------------------------------------------------------------\n",
      "[153/200] Train G Loss: 0.11347\t\n",
      "[153/200] Valiation G Loss: 0.14311\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10781333595514297 / D Loss: 0.238628089427948\n",
      "[100] G Loss: 0.11302939057350159 / D Loss: 0.2553888261318207\n",
      "-------------------------------------------------------------------\n",
      "[154/200] Train G Loss: 0.11354\t\n",
      "[154/200] Valiation G Loss: 0.14191\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11276399344205856 / D Loss: 0.24228158593177795\n",
      "[100] G Loss: 0.11358196288347244 / D Loss: 0.2519386410713196\n",
      "-------------------------------------------------------------------\n",
      "[155/200] Train G Loss: 0.11353\t\n",
      "<< model save at [155] epoch! >>\n",
      "[155/200] Valiation G Loss: 0.14281\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11713075637817383 / D Loss: 0.22453299164772034\n",
      "[100] G Loss: 0.11422836780548096 / D Loss: 0.22684286534786224\n",
      "-------------------------------------------------------------------\n",
      "[156/200] Train G Loss: 0.11356\t\n",
      "[156/200] Valiation G Loss: 0.14692\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11122245341539383 / D Loss: 0.21327555179595947\n",
      "[100] G Loss: 0.10859120637178421 / D Loss: 0.26202988624572754\n",
      "-------------------------------------------------------------------\n",
      "[157/200] Train G Loss: 0.11367\t\n",
      "[157/200] Valiation G Loss: 0.14538\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10930796712636948 / D Loss: 0.24162080883979797\n",
      "[100] G Loss: 0.11280427873134613 / D Loss: 0.2195388525724411\n",
      "-------------------------------------------------------------------\n",
      "[158/200] Train G Loss: 0.11362\t\n",
      "[158/200] Valiation G Loss: 0.14548\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11372920870780945 / D Loss: 0.2663220167160034\n",
      "[100] G Loss: 0.10974868386983871 / D Loss: 0.24609698355197906\n",
      "-------------------------------------------------------------------\n",
      "[159/200] Train G Loss: 0.11352\t\n",
      "[159/200] Valiation G Loss: 0.14297\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12326312065124512 / D Loss: 0.226780503988266\n",
      "[100] G Loss: 0.11533409357070923 / D Loss: 0.21480302512645721\n",
      "-------------------------------------------------------------------\n",
      "[160/200] Train G Loss: 0.11378\t\n",
      "<< model save at [160] epoch! >>\n",
      "[160/200] Valiation G Loss: 0.14476\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11535051465034485 / D Loss: 0.26064738631248474\n",
      "[100] G Loss: 0.11221691966056824 / D Loss: 0.2435746043920517\n",
      "-------------------------------------------------------------------\n",
      "[161/200] Train G Loss: 0.11381\t\n",
      "[161/200] Valiation G Loss: 0.14329\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11341087520122528 / D Loss: 0.24060960114002228\n",
      "[100] G Loss: 0.11113516986370087 / D Loss: 0.2335098832845688\n",
      "-------------------------------------------------------------------\n",
      "[162/200] Train G Loss: 0.11358\t\n",
      "[162/200] Valiation G Loss: 0.14283\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12159193307161331 / D Loss: 0.21465957164764404\n",
      "[100] G Loss: 0.11354337632656097 / D Loss: 0.23897084593772888\n",
      "-------------------------------------------------------------------\n",
      "[163/200] Train G Loss: 0.11371\t\n",
      "[163/200] Valiation G Loss: 0.14438\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11347771435976028 / D Loss: 0.23431994020938873\n",
      "[100] G Loss: 0.118080273270607 / D Loss: 0.2412784844636917\n",
      "-------------------------------------------------------------------\n",
      "[164/200] Train G Loss: 0.11405\t\n",
      "[164/200] Valiation G Loss: 0.14677\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11311446875333786 / D Loss: 0.24700379371643066\n",
      "[100] G Loss: 0.11518128216266632 / D Loss: 0.2352561503648758\n",
      "-------------------------------------------------------------------\n",
      "[165/200] Train G Loss: 0.11380\t\n",
      "<< model save at [165] epoch! >>\n",
      "[165/200] Valiation G Loss: 0.14348\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11089189350605011 / D Loss: 0.25912588834762573\n",
      "[100] G Loss: 0.1044168695807457 / D Loss: 0.24014601111412048\n",
      "-------------------------------------------------------------------\n",
      "[166/200] Train G Loss: 0.11365\t\n",
      "[166/200] Valiation G Loss: 0.14409\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11625814437866211 / D Loss: 0.20038965344429016\n",
      "[100] G Loss: 0.10946518927812576 / D Loss: 0.2551182210445404\n",
      "-------------------------------------------------------------------\n",
      "[167/200] Train G Loss: 0.11380\t\n",
      "[167/200] Valiation G Loss: 0.14580\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10930399596691132 / D Loss: 0.22558078169822693\n",
      "[100] G Loss: 0.10970085859298706 / D Loss: 0.22738121449947357\n",
      "-------------------------------------------------------------------\n",
      "[168/200] Train G Loss: 0.11367\t\n",
      "[168/200] Valiation G Loss: 0.14392\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1157940998673439 / D Loss: 0.25231558084487915\n",
      "[100] G Loss: 0.10999707877635956 / D Loss: 0.2568210959434509\n",
      "-------------------------------------------------------------------\n",
      "[169/200] Train G Loss: 0.11385\t\n",
      "[169/200] Valiation G Loss: 0.14282\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11359142512083054 / D Loss: 0.2400074601173401\n",
      "[100] G Loss: 0.11615505814552307 / D Loss: 0.23454324901103973\n",
      "-------------------------------------------------------------------\n",
      "[170/200] Train G Loss: 0.11368\t\n",
      "<< model save at [170] epoch! >>\n",
      "[170/200] Valiation G Loss: 0.14765\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11178561300039291 / D Loss: 0.24964278936386108\n",
      "[100] G Loss: 0.1126599982380867 / D Loss: 0.22314131259918213\n",
      "-------------------------------------------------------------------\n",
      "[171/200] Train G Loss: 0.11375\t\n",
      "[171/200] Valiation G Loss: 0.14865\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11039061844348907 / D Loss: 0.23081062734127045\n",
      "[100] G Loss: 0.106613889336586 / D Loss: 0.2610166668891907\n",
      "-------------------------------------------------------------------\n",
      "[172/200] Train G Loss: 0.11391\t\n",
      "[172/200] Valiation G Loss: 0.14623\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11338088661432266 / D Loss: 0.25962957739830017\n",
      "[100] G Loss: 0.1181340217590332 / D Loss: 0.22221580147743225\n",
      "-------------------------------------------------------------------\n",
      "[173/200] Train G Loss: 0.11372\t\n",
      "[173/200] Valiation G Loss: 0.14570\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11411136388778687 / D Loss: 0.2753930687904358\n",
      "[100] G Loss: 0.12422918528318405 / D Loss: 0.22521613538265228\n",
      "-------------------------------------------------------------------\n",
      "[174/200] Train G Loss: 0.11390\t\n",
      "[174/200] Valiation G Loss: 0.14624\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11932958662509918 / D Loss: 0.19802522659301758\n",
      "[100] G Loss: 0.1126302033662796 / D Loss: 0.25583991408348083\n",
      "-------------------------------------------------------------------\n",
      "[175/200] Train G Loss: 0.11393\t\n",
      "<< model save at [175] epoch! >>\n",
      "[175/200] Valiation G Loss: 0.14544\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10795152187347412 / D Loss: 0.24316182732582092\n",
      "[100] G Loss: 0.1074288934469223 / D Loss: 0.2574380040168762\n",
      "-------------------------------------------------------------------\n",
      "[176/200] Train G Loss: 0.11379\t\n",
      "[176/200] Valiation G Loss: 0.14325\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10404414683580399 / D Loss: 0.25240957736968994\n",
      "[100] G Loss: 0.12102597206830978 / D Loss: 0.2283555418252945\n",
      "-------------------------------------------------------------------\n",
      "[177/200] Train G Loss: 0.11392\t\n",
      "[177/200] Valiation G Loss: 0.14621\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10392280668020248 / D Loss: 0.2560117244720459\n",
      "[100] G Loss: 0.1161738783121109 / D Loss: 0.222389817237854\n",
      "-------------------------------------------------------------------\n",
      "[178/200] Train G Loss: 0.11380\t\n",
      "[178/200] Valiation G Loss: 0.14359\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10587888956069946 / D Loss: 0.23371672630310059\n",
      "[100] G Loss: 0.11373242735862732 / D Loss: 0.2338433563709259\n",
      "-------------------------------------------------------------------\n",
      "[179/200] Train G Loss: 0.11386\t\n",
      "[179/200] Valiation G Loss: 0.14363\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11218667775392532 / D Loss: 0.23951393365859985\n",
      "[100] G Loss: 0.10835044831037521 / D Loss: 0.21953120827674866\n",
      "-------------------------------------------------------------------\n",
      "[180/200] Train G Loss: 0.11377\t\n",
      "<< model save at [180] epoch! >>\n",
      "[180/200] Valiation G Loss: 0.14463\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1200614795088768 / D Loss: 0.22758188843727112\n",
      "[100] G Loss: 0.12406709045171738 / D Loss: 0.21389049291610718\n",
      "-------------------------------------------------------------------\n",
      "[181/200] Train G Loss: 0.11389\t\n",
      "[181/200] Valiation G Loss: 0.14481\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10830704867839813 / D Loss: 0.24139085412025452\n",
      "[100] G Loss: 0.11661680787801743 / D Loss: 0.20017656683921814\n",
      "-------------------------------------------------------------------\n",
      "[182/200] Train G Loss: 0.11395\t\n",
      "[182/200] Valiation G Loss: 0.14554\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11434876173734665 / D Loss: 0.21534618735313416\n",
      "[100] G Loss: 0.11150570958852768 / D Loss: 0.22297906875610352\n",
      "-------------------------------------------------------------------\n",
      "[183/200] Train G Loss: 0.11393\t\n",
      "[183/200] Valiation G Loss: 0.14698\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1170877069234848 / D Loss: 0.21851922571659088\n",
      "[100] G Loss: 0.10981602966785431 / D Loss: 0.24253085255622864\n",
      "-------------------------------------------------------------------\n",
      "[184/200] Train G Loss: 0.11399\t\n",
      "[184/200] Valiation G Loss: 0.14668\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10423336923122406 / D Loss: 0.24568314850330353\n",
      "[100] G Loss: 0.12351449579000473 / D Loss: 0.20613642036914825\n",
      "-------------------------------------------------------------------\n",
      "[185/200] Train G Loss: 0.11383\t\n",
      "<< model save at [185] epoch! >>\n",
      "[185/200] Valiation G Loss: 0.14769\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11810152977705002 / D Loss: 0.2247498333454132\n",
      "[100] G Loss: 0.11260853707790375 / D Loss: 0.2314753234386444\n",
      "-------------------------------------------------------------------\n",
      "[186/200] Train G Loss: 0.11390\t\n",
      "[186/200] Valiation G Loss: 0.14602\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11765871942043304 / D Loss: 0.2259853184223175\n",
      "[100] G Loss: 0.11281060427427292 / D Loss: 0.2256765067577362\n",
      "-------------------------------------------------------------------\n",
      "[187/200] Train G Loss: 0.11470\t\n",
      "[187/200] Valiation G Loss: 0.14788\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11301349848508835 / D Loss: 0.2513608932495117\n",
      "[100] G Loss: 0.11211216449737549 / D Loss: 0.24501760303974152\n",
      "-------------------------------------------------------------------\n",
      "[188/200] Train G Loss: 0.11445\t\n",
      "[188/200] Valiation G Loss: 0.14954\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.12021147459745407 / D Loss: 0.22410474717617035\n",
      "[100] G Loss: 0.12223084270954132 / D Loss: 0.2156217098236084\n",
      "-------------------------------------------------------------------\n",
      "[189/200] Train G Loss: 0.11397\t\n",
      "[189/200] Valiation G Loss: 0.14695\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10810334980487823 / D Loss: 0.26353660225868225\n",
      "[100] G Loss: 0.11219228059053421 / D Loss: 0.2308453619480133\n",
      "-------------------------------------------------------------------\n",
      "[190/200] Train G Loss: 0.11401\t\n",
      "<< model save at [190] epoch! >>\n",
      "[190/200] Valiation G Loss: 0.14607\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11523167043924332 / D Loss: 0.22981896996498108\n",
      "[100] G Loss: 0.10619005560874939 / D Loss: 0.25195157527923584\n",
      "-------------------------------------------------------------------\n",
      "[191/200] Train G Loss: 0.11402\t\n",
      "[191/200] Valiation G Loss: 0.14565\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1110466793179512 / D Loss: 0.23487389087677002\n",
      "[100] G Loss: 0.11100292205810547 / D Loss: 0.24463897943496704\n",
      "-------------------------------------------------------------------\n",
      "[192/200] Train G Loss: 0.11394\t\n",
      "[192/200] Valiation G Loss: 0.14658\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10691747814416885 / D Loss: 0.2474525272846222\n",
      "[100] G Loss: 0.11246196180582047 / D Loss: 0.2369145303964615\n",
      "-------------------------------------------------------------------\n",
      "[193/200] Train G Loss: 0.11391\t\n",
      "[193/200] Valiation G Loss: 0.14982\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10659372806549072 / D Loss: 0.23591187596321106\n",
      "[100] G Loss: 0.1153482049703598 / D Loss: 0.21118606626987457\n",
      "-------------------------------------------------------------------\n",
      "[194/200] Train G Loss: 0.11389\t\n",
      "[194/200] Valiation G Loss: 0.14522\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10447248816490173 / D Loss: 0.23581832647323608\n",
      "[100] G Loss: 0.10778507590293884 / D Loss: 0.251961886882782\n",
      "-------------------------------------------------------------------\n",
      "[195/200] Train G Loss: 0.11395\t\n",
      "<< model save at [195] epoch! >>\n",
      "[195/200] Valiation G Loss: 0.14308\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11738105863332748 / D Loss: 0.21603769063949585\n",
      "[100] G Loss: 0.12780071794986725 / D Loss: 0.18821892142295837\n",
      "-------------------------------------------------------------------\n",
      "[196/200] Train G Loss: 0.11393\t\n",
      "[196/200] Valiation G Loss: 0.14493\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11293987929821014 / D Loss: 0.22335827350616455\n",
      "[100] G Loss: 0.1187572032213211 / D Loss: 0.24566665291786194\n",
      "-------------------------------------------------------------------\n",
      "[197/200] Train G Loss: 0.11397\t\n",
      "[197/200] Valiation G Loss: 0.14514\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.11975739151239395 / D Loss: 0.2235751450061798\n",
      "[100] G Loss: 0.11523960530757904 / D Loss: 0.2523525059223175\n",
      "-------------------------------------------------------------------\n",
      "[198/200] Train G Loss: 0.11406\t\n",
      "[198/200] Valiation G Loss: 0.14386\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.10986565053462982 / D Loss: 0.23948770761489868\n",
      "[100] G Loss: 0.11167269945144653 / D Loss: 0.26553601026535034\n",
      "-------------------------------------------------------------------\n",
      "[199/200] Train G Loss: 0.11422\t\n",
      "[199/200] Valiation G Loss: 0.14345\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[50] G Loss: 0.1198296919465065 / D Loss: 0.2624298334121704\n",
      "[100] G Loss: 0.11056896299123764 / D Loss: 0.2386666238307953\n",
      "-------------------------------------------------------------------\n",
      "[200/200] Train G Loss: 0.11393\t\n",
      "<< model save at [200] epoch! >>\n",
      "[200/200] Valiation G Loss: 0.14849\t\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "<< Finished Training >>\n",
      "<< Last Model Saved >>\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(epochs):\n",
    "    train_sum_loss = 0\n",
    "    val_sum_loss = 0  \n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        nd, qd = data\n",
    " \n",
    "        input = qd.to(device) # qd = input\n",
    "\n",
    "        target1 = (nd).to(device) # nd = target1\n",
    "        target2 = (nd-qd).to(device) # difference map = target2\n",
    "        target = torch.cat([target1, target2], dim=1)\n",
    " \n",
    "        # train G\n",
    "        out_g = generator(input) # predict nd \n",
    "        sub_g = out_g - input # predict diff map \n",
    "        pred = torch.cat([out_g, sub_g], dim=1).to(device)\n",
    "\n",
    "        fake = discriminator(pred) # Fake [pred nd, pred diff] <for adv learning> \n",
    "        loss_gen = getG_loss(out=out_g, sub=sub_g, out_target=target1, sub_target=target2, fake=fake, coefs=coefs)\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_g.step()\n",
    "        train_sum_loss += loss_gen.item()\n",
    "\n",
    "        # train D\n",
    "        out_d1 = discriminator(target) # Real [nd, diff]\n",
    "        out_d2 = discriminator(pred.detach()) # Fake [pred nd, pred diff]\n",
    "\n",
    "        loss_dis = getD_loss(real=out_d1, fake=out_d2)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        loss_dis.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # check loss per 50 iteration\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"[{i+1}] G Loss: {loss_gen.item()} / D Loss: {loss_dis.item()}\")\n",
    "\n",
    "    # record train loss\n",
    "    tr_loss = train_sum_loss / len(train_loader)\n",
    "    train_g_losses.append(tr_loss)\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('[%d/%d] Train G Loss: %.5f\\t'% (epoch+1, epochs, tr_loss))\n",
    "\n",
    "    # validation per one epoch\n",
    "    for j, data_v in enumerate(valid_loader, 0):\n",
    "        nd_v, qd_v = data_v\n",
    "\n",
    "        input_v = qd_v.to(device) # qd_v = input_v\n",
    "\n",
    "        target1_v = (nd_v).to(device) # nd_v = target1_v\n",
    "        target2_v = (nd_v-qd_v).to(device) # difference map = target2_v\n",
    "\n",
    "        out_v = generator(input_v) # predict nd_v\n",
    "        sub_v = out_v - input_v # predict diff map\n",
    "        pred_v = torch.cat([out_v, sub_v], dim=1)\n",
    "        fake_v = discriminator(pred_v) # Fake [pred nd, pred diff] <for adv learning> \n",
    "\n",
    "        loss_v = getG_loss(out=out_v, sub=sub_v, out_target=target1_v, sub_target=target2_v, fake=fake_v, coefs=coefs)\n",
    "        val_sum_loss += loss_v.item()\n",
    "\n",
    "    # save model\n",
    "    val_loss = val_sum_loss / len(valid_loader)\n",
    "    if (epoch+1) >= 10:\n",
    "        # early stopping\n",
    "        if val_loss < min(valid_g_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(generator.state_dict(), BEST_SAVE_PATH)\n",
    "            print('<< Best model save at [%d] epoch! >>' % (epoch+1))\n",
    "        # save per 5 epoch\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(generator.state_dict(), SAVE_PATH+f\"_{epoch+1}.pth\")\n",
    "            print('<< model save at [%d] epoch! >>' % (epoch+1))\n",
    "\n",
    "    # record validation loss\n",
    "    valid_g_losses.append(val_loss)\n",
    "    print('[%d/%d] Valiation G Loss: %.5f\\t'% (epoch+1, epochs, val_loss))\n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "\n",
    "print('<< Finished Training >>')\n",
    "torch.save(generator.state_dict(), LAST_SAVE_PATH)\n",
    "print(\"<< Last Model Saved >>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Denoising (GAN)\")\n",
    "plt.plot(train_g_losses,label=\"train loss\")\n",
    "plt.plot(valid_g_losses,label=\"val loss\")\n",
    "plt.axvline(best_epoch, color='green', linestyle='--', linewidth=2, label=f\"early stopping ({best_epoch})\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
